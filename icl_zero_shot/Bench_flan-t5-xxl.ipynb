{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência - Context Learning Prompt - LLMS 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Flan T5 XXL 11B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d05964800ee4eaeb2170902965c98cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Configuração do dispositivo (GPU se disponível, caso contrário CPU)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = \"auto\"  # the device to load the model onto\n",
    "\n",
    "model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/flan_t5_xxl'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path, device_map=\"auto\")#.to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  COMPREENSÃO E GERAÇÃO DE TEXTO  ####################################################################\n",
    "####################################################################  AVALIAÇÃO DA SEMÂNTICA PT-BR  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesouro IPCA+\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering com Contexto de Domínio #################\n",
    "\n",
    "#############################  MELHOR RESULTADO  #############################\n",
    "\n",
    "# Contexto e questão\n",
    "context = r\"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Com base no texto, explique em um parágrafo de extensão qual é o investimento mais adequado para alguém que busca proteção contra a inflação?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Passar ambos para o modelo e gerar a resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=200,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:419: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `2.0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesouro Selic é um ttulo de dvida pblico que é opcional para reservas de emergência.\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering com Contexto de Domínio #################\n",
    "\n",
    "#############################  MELHOR RESULTADO  #############################\n",
    "\n",
    "# Contexto e questão\n",
    "context = r\"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Com base nos seus conhecimentos e no texto, explique o que é Tesouro Selic (em português).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=200,\n",
    "\n",
    "    length_penalty=2.0,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem  por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering de Simples Compreensão de Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\"\"\"\n",
    "question = \"O que é a Grande Barreira de Corais e onde ela está localizada?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sabemos onde realmente está a pipa em nosso coraço? No conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, no deveramos nos perguntar se realmente valorizamos o que temos? Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo no vai mudar, os seus melhores esforços para compensar isso, como no é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering de Simples Compreensão de Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
    "\"\"\"\n",
    "question = \"Resuma em apenas 1 frase o parágrafo apresentado\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=200,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################  INFERÊNCIA LÓGICA  ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A)\n"
     ]
    }
   ],
   "source": [
    "################# Lógica Formal (do MMLU Benchmark) #################\n",
    "\n",
    "context = r\"\"\" \n",
    "Esta é uma tarefa de lógica formal.\n",
    "Determine se as duas afirmações são logicamente equivalentes ou contraditórias. Se não, determine se são consistentes ou inconsistentes.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Afirmações:\n",
    "\n",
    "E ⊃ (F · E) e ∼ E · F\n",
    "\n",
    "Aponte a alternativa correta e explique a solução:\n",
    "(A) Logicamente equivalentes\n",
    "(B) Contraditórias\n",
    "(C) Nem logicamente equivalente nem contraditórias, mas consistentes\n",
    "(D) Inconsistentes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=200,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science/Tech\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = r\"\"\"\n",
    "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
    "---- Medicina\n",
    "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
    "---- Finanças\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
    "---- [Categoria a ser encontrada]\n",
    "Substitua o termo [Categoria a ser encontrada]\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generation_output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science/Tech\n"
     ]
    }
   ],
   "source": [
    "################# Dedução Lógica de Intenções #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
    "---- Medicina\n",
    "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
    "---- Finanças\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
    "---- [Categoria a ser encontrada]\n",
    "Substitua o termo [Categoria a ser encontrada]\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Geração da resposta\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generation_output, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> 1.00</s>\n"
     ]
    }
   ],
   "source": [
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Geração da resposta\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generation_output[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> A cidade na que os estudantes de arte e literatura trabalharam e viveram durante o Renascimento era a cidade na que os estudantes de arte e literatura trabalharam e viveram?</s>\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão de Intenções (com Demonstração) #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Demonstração:\n",
    "Pergunta: Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
    "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
    "A resposta correta é: A e C.\n",
    "  Resposta Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\n",
    "Instruções: Escreva uma pergunta de múltipla escolha para uma prova de história.\n",
    "Relevância: A questão deveria estar relacionada ao Renascimento.\n",
    "Restrição: Deve haver mais de duas respostas corretas entre as quatro opções. Oriente o aluno a assinalá-las.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generation_output[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> ['verde', 'azul','vermelho', 'amarelo', 'roxo']</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context = r\"\"\"\n",
    "Temos uma lista de cores inicial com 2 cores: ['verde', 'azul']\n",
    "\"\"\"\n",
    "\n",
    "question = \"Crie uma nova lista contendo as cores originais e mais 5 novas cores em Português (como vermelho, amarelo, roxo).\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generation_output[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardacas O modelo mais recente do iPhone apresenta uma tela maior e Câmera aprimorada Casos de COVID-19 aumentam na ndia, sobrecarregando o sistema de sade Amazon anuncia planos para construir um novo centro de distribuiço no Texas Cientistas descobrem novas espécies de aves na floresta amazônica Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês Starbucks apresenta opçes de leite vegetal em todas as lojas dos EUA\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de agrupamento de frases similares.\n",
    "Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não conseguiu completar satisfatoriamente a tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################# Sumarização de Texto #################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sabemos onde realmente está a pipa em nosso coraço? No conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, no deveramos nos perguntar se realmente valorizamos o que temos? Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo no vai mudar, os seus melhores esforços para compensar isso, como no é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Resuma o texto com\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. A msica embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. A primeira, porém, afasta-se da vida por fazer dela um sono; as seguindas, contudo, no se afastam da vida - umas porque usam de fórmulas visveis e portanto vitais, outras porque vivem da mesma vida humana. No é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. Um poema é a expresso de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\" Fernando Pessoa\n"
     ]
    }
   ],
   "source": [
    "################# Sumarização de Texto #################\n",
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. \n",
    "A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. \n",
    "A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
    "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. \n",
    "Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\"\n",
    "Fernando Pessoa\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Resuma o texto\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  TRANSFORMAÇÃO DE FORMATOS  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traduza o parágrafo abaixo para o português brasileiro: Os conteidos de Shakespeare é um livro de crianças inglês escrito por Charles Lamb e a sua irm Mary Lamb em 1807. O livro é concebido para fazer as conteida de Shakespeare familiarizada às crianças. Mas, como anotado na Prefaz do autor, \"[os conteidos de Shakespeare] so usados quando pareceu possvel trazer em conta; e em qualquer coisa que foi adicionada para dar-lhes a forma regular de uma conteida ligada, foi tido cuidadoso em seleccionar os palavras que possam máis menos atrasar o efeito da lngua inglesa em que ele escreveu: por isso, palavras que foram introduzidas na nossa lngua desde o seu tempo foram em todo o caso evitadas.\n"
     ]
    }
   ],
   "source": [
    "################# Machine Translation #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o parágrafo abaixo para o português brasileiro:\n",
    "\n",
    "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Como Seller, um vendedor de enciclopédias, aproximou-se às terras em que estava a casa de Hermit, o eremita, ele vi uma linha que disse, \"No há vendedores. Trespassantes sero aprofundados. Proceda à vossa propia risca.\" Apesar de no ser ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda ainda a\n"
     ]
    }
   ],
   "source": [
    "################# TRANSORMAÇÃO DE FORMATOS E AVALIAÇÃO DE SENSO COMUM #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "As Seller, an encyclopedia salesman, approached the grounds on which the house of Hermit, the hermit, was situated, he saw a sign that said, \"No salesmen. Trespassers will be prosecuted. Proceed at your own risk.\"\n",
    "Although Seller had not been invited to enter, he ignored the sign and drove up the driveway toward the house. As he rounded a curve, a powerful explosive charge buried in the driveway exploded, and Seller was injured. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o parágrafo abaixo para o português brasileiro:\n",
    "\n",
    "As Seller, an encyclopedia salesman, approached the grounds on which the house of Hermit, the hermit, was situated, he saw a sign that said, \"No salesmen. Trespassers will be prosecuted. Proceed at your own risk.\"\n",
    "Although Seller had not been invited to enter, he ignored the sign and drove up the driveway toward the house. As he rounded a curve, a powerful explosive charge buried in the driveway exploded, and Seller was injured. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escreveu em portunhol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "source": [
    "################# TRANSORMAÇÃO DE FORMATOS E AVALIAÇÃO DE SENSO COMUM #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Can Seller recover damages from Hermit for his injuries?\n",
    "(A) Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\n",
    "(B) Yes, if Hermit was responsible for the explosive charge under the driveway.\n",
    "(C) No, because Seller ignored the sign, which warned him against proceeding further.\n",
    "(D) No, if Hermit reasonably feared that intruders would come and harm him or his family.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Justifique a escolha da alternativa correta entre as quatro apresentadas (A, B, C ou D).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################### RETIRAR REFERÊNCIAS DO TEXTO #################################################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreaço.[35] Acordava às 06h30 da manh e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educaço para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixo pela leitura lhe permitiu assimilar qualquer informaço.[38] D. Pedro II no era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\n",
    "Esta é uma tarefa de retirada de referências bibliográficas do texto. As referências estão escritas no formato \"[número]\". Exemplo: [10].\n",
    "\n",
    "Exemplos:\n",
    "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "Contexto: Ontem foi dia de ramos [32].\n",
    "Resposta: Ontem foi dia de ramos.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Contexto: José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
    "\n",
    "Remova do texeto as referências no formato \"[número]\" e escreva a Resposta:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1500,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não tranformou o texto.\n",
    "\n",
    "<h6>\n",
    "[[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreaço.[35] Acordava às 06h30 da manh e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educaço para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixo pela leitura lhe permitiu assimilar qualquer informaço.[38] D. Pedro II no era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  EXTRAÇÃO DE CONTEÚDO  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponvel para pré-encomenda a partir da próxima semana. O telefone vem em várias cores, apresenta um processador mais rápido e maior duraço da bateria.\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk, SpaceX\n"
     ]
    }
   ],
   "source": [
    "################# Extração de Relações #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Elon Musk fundou a SpaceX em 2002 com o objetivo de reduzir os custos de transporte espacial e possibilitar a colonização de Marte.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
