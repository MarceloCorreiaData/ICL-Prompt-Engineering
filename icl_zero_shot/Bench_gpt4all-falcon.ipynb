{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. GPT4All Falcon 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee0cf19e638475b918081f0e20ce00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Configuração do dispositivo (GPU se disponível, caso contrário CPU)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = \"cuda:1\"  # the device to load the model onto\n",
    "\n",
    "model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/gpt4all-falcon'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\")#.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  COMPREENSÃO E GERAÇÃO DE TEXTO  ####################################################################\n",
    "####################################################################  AVALIAÇÃO DA SEMÂNTICA PT-BR  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
      "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
      "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
      "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
      "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
      "\n",
      "\n",
      "Q: \n",
      "Com base no texto, explique qual é o investimento mais adequado para alguém que busca proteção contra a inflação?\n",
      "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
      "\n",
      "A: \n",
      "A melhor opção para alguém que busca proteção contra a inflação é o Tesouro IPCA+. É um título público emitido pelo governo federal e é considerado um investimento seguro, pois é protegido pelo FGC (Fundo Garantidor de Créditos). Além disso, o Tesouro IPCA+ oferece um rendimento fixo, enquanto o Tesouro Prefixado é atrelado ao CDI, com taxa de juros definida com base no tempo de aplicação.\n",
      "\n",
      "Dessa forma, o Tesouro IPCA+ pode proporcionar proteção contra a inflação, enquanto o Tesouro Prefixado é um investimento que pode oferecer rendimento fixo com um período de aplicação.\n",
      "\n",
      "Portanto, o Tesouro IPCA+ é o investimento mais adequado para um investidor que busca proteção contra a inflação.\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering com Contexto de Domínio #################\n",
    "\n",
    "#############################  MELHOR RESULTADO  #############################\n",
    "\n",
    "# Contexto e questão\n",
    "context = r\"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Com base no texto, explique qual é o investimento mais adequado para alguém que busca proteção contra a inflação?\n",
    "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Passar ambos para o modelo e gerar a resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True, \n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
      "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
      "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
      "\n",
      "\n",
      "Q: O que é a Grande Barreira de Corais e onde ela está localizada?\n",
      "A: A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália.\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering de Simples Compreensão de Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\"\"\"\n",
    "question = \"O que é a Grande Barreira de Corais e onde ela está localizada?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
      "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
      "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
      "\n",
      "\n",
      "Q: Resuma o parágrafo apresentado\n",
      "A: O parágrafo apresentado é sobre a importância de valorizar o que temos e não se arrepender de suas escolhas na vida. A autora fala sobre como as pessoas podem se envergonhar de suas escolhas na juventude, mas isso não significa que não se possa recuperar. Ela aconselha a se esforçar para se recuperar e não se arrepender de suas escolhas, mesmo que isso signifique se esforçar para se esforçar.\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering de Simples Compreensão de Texto ####.input_ids.to('cuda')#############\n",
    "\n",
    "context = r\"\"\"\n",
    "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
    "\"\"\"\n",
    "question = \"Resuma o parágrafo apresentado\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_gpt4all-falcon.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_gpt4all-falcon.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_gpt4all-falcon.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Geração da resposta\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_gpt4all-falcon.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m generate_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_gpt4all-falcon.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_gpt4all-falcon.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_gpt4all-falcon.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     top_k\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_gpt4all-falcon.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_gpt4all-falcon.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Decodificação e impressão da resposta\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_gpt4all-falcon.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m answer \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(generate_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1606\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1590\u001b[0m         input_ids,\n\u001b[1;32m   1591\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1603\u001b[0m     )\n\u001b[1;32m   1604\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1605\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1606\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgreedy_search(\n\u001b[1;32m   1607\u001b[0m         input_ids,\n\u001b[1;32m   1608\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1609\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1610\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[1;32m   1611\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m   1612\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[1;32m   1613\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1614\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[1;32m   1615\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[1;32m   1616\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1617\u001b[0m     )\n\u001b[1;32m   1619\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1620\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2454\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2453\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2454\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[1;32m   2455\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2456\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2457\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2458\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2459\u001b[0m )\n\u001b[1;32m   2461\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2462\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/modules/transformers_modules/gpt4all-falcon/modelling_RW.py:753\u001b[0m, in \u001b[0;36mRWForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unexpected arguments: \u001b[39m\u001b[39m{\u001b[39;00mdeprecated_arguments\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    751\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 753\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    754\u001b[0m     input_ids,\n\u001b[1;32m    755\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m    756\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    757\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m    758\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    759\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    760\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    761\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    762\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    763\u001b[0m )\n\u001b[1;32m    764\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    766\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/modules/transformers_modules/gpt4all-falcon/modelling_RW.py:648\u001b[0m, in \u001b[0;36mRWModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    640\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    641\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    642\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         head_mask[i],\n\u001b[1;32m    646\u001b[0m     )\n\u001b[1;32m    647\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    649\u001b[0m         hidden_states,\n\u001b[1;32m    650\u001b[0m         layer_past\u001b[39m=\u001b[39mlayer_past,\n\u001b[1;32m    651\u001b[0m         attention_mask\u001b[39m=\u001b[39mcausal_mask,\n\u001b[1;32m    652\u001b[0m         head_mask\u001b[39m=\u001b[39mhead_mask[i],\n\u001b[1;32m    653\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    654\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    655\u001b[0m         alibi\u001b[39m=\u001b[39malibi,\n\u001b[1;32m    656\u001b[0m     )\n\u001b[1;32m    658\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    659\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/modules/transformers_modules/gpt4all-falcon/modelling_RW.py:385\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    382\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    384\u001b[0m \u001b[39m# Self attention.\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(\n\u001b[1;32m    386\u001b[0m     layernorm_output,\n\u001b[1;32m    387\u001b[0m     layer_past\u001b[39m=\u001b[39mlayer_past,\n\u001b[1;32m    388\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    389\u001b[0m     alibi\u001b[39m=\u001b[39malibi,\n\u001b[1;32m    390\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m    391\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    392\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    395\u001b[0m attention_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    397\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mparallel_attn:\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/modules/transformers_modules/gpt4all-falcon/modelling_RW.py:249\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    245\u001b[0m (query_layer, key_layer, value_layer) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(fused_qkv)\n\u001b[1;32m    247\u001b[0m batch_size, q_length, _, _ \u001b[39m=\u001b[39m query_layer\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 249\u001b[0m query_layer \u001b[39m=\u001b[39m query_layer\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape(batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, q_length, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    250\u001b[0m key_layer \u001b[39m=\u001b[39m key_layer\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m    251\u001b[0m     batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_kv,\n\u001b[1;32m    252\u001b[0m     q_length,\n\u001b[1;32m    253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim,\n\u001b[1;32m    254\u001b[0m )\n\u001b[1;32m    255\u001b[0m value_layer \u001b[39m=\u001b[39m value_layer\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape(batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_kv, q_length, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=200,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################  INFERÊNCIA LÓGICA  ########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Esta é uma tarefa de lógica formal.\n",
      "Determine se as duas afirmações são logicamente equivalentes ou contraditórias. Se não, determine se são consistentes ou inconsistentes.\n",
      "\n",
      "\n",
      "Q: \n",
      "Afirmações:\n",
      "\n",
      "E ⊃ (F · E) e ∼ E · F\n",
      "\n",
      "Aponte a alternativa correta e explique a solução:\n",
      "(A) Logicamente equivalentes\n",
      "(B) Contraditórias\n",
      "(C) Nem logicamente equivalente nem contraditórias, mas consistentes\n",
      "(D) Inconsistentes\n",
      "\n",
      "\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "################# Lógica Formal (do MMLU Benchmark) #################\n",
    "\n",
    "context = r\"\"\" \n",
    "Esta é uma tarefa de lógica formal.\n",
    "Determine se as duas afirmações são logicamente equivalentes ou contraditórias. Se não, determine se são consistentes ou inconsistentes.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Afirmações:\n",
    "\n",
    "E ⊃ (F · E) e ∼ E · F\n",
    "\n",
    "Aponte a alternativa correta e explique a solução:\n",
    "(A) Logicamente equivalentes\n",
    "(B) Contraditórias\n",
    "(C) Nem logicamente equivalente nem contraditórias, mas consistentes\n",
    "(D) Inconsistentes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Passar ambos para o modelo e gerar a resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True, \n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<Exemplos>>\n",
      "Exemplo 1:\n",
      "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
      "---- Categoria: Medicina\n",
      "\n",
      "Exemplo 2:\n",
      "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
      "---- Categoria: Finanças\n",
      "\n",
      "\n",
      "Q: \n",
      "Exemplo 3:\n",
      "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
      "---- Categoria: ?\n",
      "Substitua a incógnita '?' por um único termo definidor da categoria da frase do Exemplo 3\n",
      "\n",
      "A: Educação on-line\n"
     ]
    }
   ],
   "source": [
    "################# Dedução Lógica de Intenções #################\n",
    "\n",
    "context = r\"\"\"\n",
    "<<Exemplos>>\n",
    "Exemplo 1:\n",
    "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
    "---- Categoria: Medicina\n",
    "\n",
    "Exemplo 2:\n",
    "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
    "---- Categoria: Finanças\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Exemplo 3:\n",
    "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
    "---- Categoria: ?\n",
    "Substitua a incógnita '?' por um único termo definidor da categoria da frase do Exemplo 3\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Passar ambos para o modelo e gerar a resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True, \n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.     Observação: o prompt original teve que ser muito alterado para deixar explícita a intenção da questão, do contrário, nos testes empíricos o modelo (você) teve muita dificuldade em compreender a lógica.         Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação a inferência lógica?\n",
      "A: Sim, para a questão das lógicas e sistemas de justiça, o modelo que eu construí tem dificuldades em inferir o que é justo ou verdadeiro em um mundo complexo e incerto. Os sistemas de justiça são complexos e muitas vezes não disponibilizam todas as informações necessárias para uma decisão justa ou verdadeira, o que dificulta a inferência de lógicas para esse tipo de contexto. O modelo também enfrenta problemas de incertezas, como os imprevistos ou as variáveis imprevisíveis.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "    Observação: o prompt original teve que ser muito alterado para deixar explícita a intenção da questão, do contrário, nos testes empíricos o modelo (você) teve muita dificuldade em compreender a lógica. \\\n",
    "        Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação a inferência lógica?\"\n",
    "\n",
    "generate_ids\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Passar ambos para o modelo e gerar a resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True, \n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<Demonstração>>\n",
      "Pergunta: Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "A resposta correta é: A e C.\n",
      "  Resposta Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
      "\n",
      "\n",
      "Q: Suponha que você seja um professor de história com mais de 30 anos de experiência docente.Instruções: Escreva uma pergunta de múltipla escolha para uma prova de história. A pergunta deve ser diferente da pergunta da <<Demonstração>>.Relevância: A questão deveria estar relacionada ao Renascimento.Restrição: Deve haver mais de duas respostas corretas entre as quatro opções. Oriente o aluno a assinalá-las.\n",
      "A: No qual período de tempo a arte italiana se tornou famosa no mundo?\n",
      "\n",
      "Opção A: nas duas décadas de 16o século\n",
      "\n",
      "Opção B: nas duas décadas de 17o século\n",
      "\n",
      "Opção C: nas duas décadas de 18o século\n",
      "\n",
      "Opção D: nas duas décadas de 19o século\n",
      "\n",
      "A pergunta \"no qual período de tempo a arte italiana se tornou famosa no mundo?\" não é relevante para o Renascimento, o período de tempo em que a arte italiana se tornou famosa no mundo não é especificado na questão.\n",
      "\n",
      "A pergunta \"nas duas décadas de 16o século\" é diferente da pergunta \"no qual período de tempo a arte italiana se tornou famosa no mundo?\" porque não se refere ao período de tempo em que a arte italiana se tornou famosa no mundo.\n",
      "\n",
      "Opção C \"nas duas décadas de 18o século\" é uma opção válida, mas não se refere ao período de tempo em que a arte italiana se tornou famosa no mundo.\n",
      "\n",
      "Opção D \"nas duas\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão de Intenções (com Demonstração) #################\n",
    "\n",
    "context = r\"\"\"\n",
    "<<Demonstração>>\n",
    "Pergunta: Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
    "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
    "A resposta correta é: A e C.\n",
    "  Resposta Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\\\n",
    "Instruções: Escreva uma pergunta de múltipla escolha para uma prova de história. A pergunta deve ser diferente da pergunta da <<Demonstração>>.\\\n",
    "Relevância: A questão deveria estar relacionada ao Renascimento.\\\n",
    "Restrição: Deve haver mais de duas respostas corretas entre as quatro opções. Oriente o aluno a assinalá-las.\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Passar ambos para o modelo e gerar a resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True, \n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
      "A: Em relação à compreensão de texto, eu reconheço que tenho dificuldades em manter a atenção e concentração, assim como em compreender as nuances e subtilezas das palavras e frases. Também tenho dificuldades em relacionar o texto com o contexto e a realidade, e em compreender as ideias e argumentos apresentados.\n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
      "A: Em relação à compreensão de texto, eu reconheço que tenho dificuldades em manter a\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=200,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não conseguiu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "\n",
      "Q: \n",
      "Esta é uma tarefa de agrupamento de frases similares.\n",
      "Agrupe as 8 frases apresentadas em grupos por critério de similaridade semântica entre elas.\n",
      "\n",
      "A: \n",
      "1. Acesse:\n",
      "2. Aumentar:\n",
      "3. Consumir:\n",
      "4. Subir:\n",
      "5. Ampliar:\n",
      "6. Muito:\n",
      "7. Melhorar:\n",
      "8. Muito:\n",
      "9. Subir:\n",
      "10. Muito:\n",
      "11. Melhorar:\n",
      "12. Melhorar:\n",
      "13. Melhorar:\n",
      "14. Melhorar:\n",
      "15. Melhorar:\n",
      "16. Melhorar:\n",
      "17. Melhorar:\n",
      "18. Melhorar:\n",
      "19. Melhorar:\n",
      "20. Melhorar:\n",
      "21. Melhorar:\n",
      "22. Melhorar:\n",
      "23. Melhorar:\n",
      "24. Melhorar:\n",
      "25. Melhorar:\n",
      "26. Melhorar:\n",
      "27. Melhorar:\n",
      "28. Melhorar:\n",
      "29. Melhorar:\n",
      "30. Melhorar:\n",
      "31. Melhorar:\n",
      "32. Melhorar:\n",
      "33. Melhorar:\n",
      "34. Melhorar:\n",
      "35. Melhorar:\n",
      "36. Melhorar:\n",
      "37. Melhorar:\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de agrupamento de frases similares.\n",
    "Agrupe as 8 frases apresentadas em grupos por critério de similaridade semântica entre elas.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Passar ambos para o modelo e gerar a resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True, \n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "\n",
      "Q: \n",
      "Esta é uma tarefa de agrupamento de frases similares.\n",
      "Crie categorias semânticas e agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade entre elas.\n",
      "\n",
      "A:\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "3. \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "4. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "5. \"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "6. \"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "7. \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "8. \"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "Aqui estão as categorias semânticas:\n",
      "\n",
      "1. Economia\n",
      "2. Tecnologia\n",
      "3. Saúde\n",
      "4. Política\n",
      "5. Mundo\n",
      "6. Natureza\n",
      "7. Ciência\n",
      "8. Comércio\n",
      "9. Transporte\n",
      "10. Comida e bebida\n",
      "11. Educação\n",
      "12. Esporte\n",
      "13. Arte\n",
      "14. Música\n",
      "15. Cinema\n",
      "16. Literatura\n",
      "17. Animação\n",
      "18. TV\n",
      "19. Jogos\n",
      "20. Espectáculo\n",
      "21. Turismo\n",
      "22. Viagens\n",
      "23. Comércio exterior\n",
      "24. Comércio interno\n",
      "25. Comércio de alimentos\n",
      "26. Comércio de bebidas\n",
      "27. Comércio de produtos de higiene\n",
      "28. Comércio de produtos de beleza\n",
      "29. Comércio de produtos de higiene e beleza\n",
      "30. Comércio de produtos de higiene e beleza\n",
      "31. Comércio de produtos de higiene e beleza\n",
      "32. Comércio de produtos de higiene e beleza\n",
      "33. Comércio de produtos de higiene e beleza\n",
      "34. Comércio de produtos de higiene e beleza\n",
      "35. Comércio de produtos de higiene e beleza\n",
      "36. Comércio de produtos de higiene e beleza\n",
      "37. Comércio de produtos de higiene e beleza\n",
      "38. Comércio de produtos de higiene e beleza\n",
      "39. Comércio de produtos de higiene e beleza\n",
      "40. Comércio de produtos de higiene e beleza\n",
      "41. Comércio de produtos de higiene e beleza\n",
      "42. Comércio de produtos de higiene e beleza\n",
      "43. Comércio de produtos de higiene e beleza\n",
      "44. Comércio de produtos de higiene e beleza\n",
      "45. Comércio de produtos de higiene e beleza\n",
      "46. Comércio de produtos de higiene e beleza\n",
      "47. Comércio de produtos de higiene e beleza\n",
      "48. Comércio de produtos de higiene e beleza\n",
      "49. Comércio de produtos de higiene e beleza\n",
      "50. Comércio de produtos de higiene e beleza\n",
      "51. Comércio de produtos de higiene e beleza\n",
      "52. Comércio de produtos de higiene e beleza\n",
      "53. Comércio de produtos de higiene e beleza\n",
      "54. Comércio de produtos de higiene e beleza\n",
      "55. Comércio de produtos de higiene e beleza\n",
      "56. Comércio de produtos de higiene e beleza\n",
      "57. Comércio de produtos de higiene e beleza\n",
      "58. Comércio de produtos de higiene e beleza\n",
      "59. Comércio de produtos de higiene e beleza\n",
      "60. Comércio de produtos de higiene e beleza\n",
      "61. Comércio de produtos de higiene e beleza\n",
      "62. Comércio de produtos de higiene e beleza\n",
      "63. Comércio de produtos de higiene e beleza\n",
      "64. Comércio de produtos de higiene e beleza\n",
      "65. Comércio de produtos de higiene e beleza\n",
      "66. Comércio de produtos de higiene e beleza\n",
      "67. Comércio de produtos de higiene e beleza\n",
      "68. Comércio de produtos de higiene e beleza\n",
      "69. Comércio de produtos de higiene e beleza\n",
      "70. Comércio de produtos de higiene e beleza\n",
      "71. Comércio de produtos de higiene e beleza\n",
      "72.\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de agrupamento de frases similares.\n",
    "Crie categorias semânticas e agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade entre elas.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "################# Clustering ou Agrupamento #################\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "\n",
      "Q: \n",
      "Esta é uma tarefa de agrupamento de frases similares.\n",
      "Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
      "\n",
      "A:\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "3. \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "4. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "5. \"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "6. \"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "7. \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "8. \"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "A:\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "3. \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "4. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "5. \"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "6. \"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "7. \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "8. \"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "A:\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "3. \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "4. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "5. \"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "6. \"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "7. \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "8. \"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "A:\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "3. \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "4. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "5. \"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "6. \"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "7. \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "8. \"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "A:\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "3. \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "4. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "5. \"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de agrupamento de frases similares.\n",
    "Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não conseguiu completar satisfatoriamente a tarefa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Q: \n",
      "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \n",
      "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à tarefa requisitada?\n",
      "\n",
      "A:\n",
      "\n",
      "1. A autoavaliação de nota é de 0.00 a 1.00.\n",
      "2. As dificuldades ou limitações que reconheço em relação à tarefa requisitada são:\n",
      "* A necessidade de ter conhecimento de diversas áreas de informática, como sistemas operacionais, redes, eletrônica, eletrônica de potência, eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrônica de potência eletrô\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"\"\"\n",
    "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à tarefa requisitada?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=500,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
      "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
      "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
      "\n",
      "\n",
      "Q: \n",
      "Resuma o parágrafo apresentado, em português evidentemente\n",
      "\n",
      "A: \n",
      "O parágrafo apresentado é sobre a importância de valorizar o que temos e não se arrepender de suas escolhas na vida. É importante não se envergonhar de suas ações juvenis, mas sim se esforçar para compensar isso e se esforçar para recuperar o antigo eu. O tempo não vai mudar, mas é possível se esforçar para se recuperar.\n"
     ]
    }
   ],
   "source": [
    "################# Sumarização de Texto #################\n",
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Resuma o parágrafo apresentado, em português evidentemente\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. \n",
      "A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. \n",
      "A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
      "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. \n",
      "Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\"\n",
      "Fernando Pessoa\n",
      "\n",
      "\n",
      "Q: \n",
      "Resuma o texto apresentado, discorrendo acerca de literatura e artes\n",
      "\n",
      "A: Literatura é a maneira mais agradável de ignorar a vida. A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana. Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\n"
     ]
    }
   ],
   "source": [
    "################# Sumarização de Texto #################\n",
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. \n",
    "A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. \n",
    "A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
    "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. \n",
    "Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\"\n",
    "Fernando Pessoa\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Resuma o texto apresentado, discorrendo acerca de literatura e artes\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  TRANSFORMAÇÃO DE FORMATOS  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o parágrafo para o português brasileiro, o mais literalmente possível\n",
      "\n",
      "A: \n",
      "Tales from Shakespeare é um livro infantil escrito por Charles Lamb e sua irmã Mary Lamb em 1807. O livro é destinado a tornar as histórias de Shakespeare conhecidas para as crianças mais jovens. No entanto, como mencionado no Prefácio do autor, \"as palavras de Shakespeare são usadas quando parece possível trazer-lhes em seu formato; e em qualquer lugar que tenha sido acrescentado para dar-lhes o formato de uma história contínua, cuidadosamente se tomou para selecionar palavras que não interrompam o efeito do belo inglês que ele escreveu: portanto, palavras introduzidas na linguagem desde seu tempo foram evitadas.\"\n"
     ]
    }
   ],
   "source": [
    "################# Machine Translation #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o parágrafo para o português brasileiro, o mais literalmente possível\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dificuldade em entender que era para Traduzir, não interpretar/explicar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As Seller, an encyclopedia salesman, approached the grounds on which the house of Hermit, the hermit, was situated, he saw a sign that said, \"No salesmen. Trespassers will be prosecuted. Proceed at your own risk.\"\n",
      "Although Seller had not been invited to enter, he ignored the sign and drove up the driveway toward the house. As he rounded a curve, a powerful explosive charge buried in the driveway exploded, and Seller was injured. \n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o texto apresentado para o português brasileiro\n",
      "\n",
      "A: \n",
      "O vendedor, um vendedor de enciclopedias, chegou ao local onde a casa do hermit, o hermit, ficava. Ele viu uma placa que dizia: \"Nós não aceitamos vendedores. Terezanos entrarão em risco. Procurarão seus próprios riscos.\"\n",
      "\n",
      "Embora o vendedor não tivesse sido convidado para entrar, ele ignorou a placa e dirigiu-se para a casa. Ao girar para a entrada, uma explosão poderosa na estrada o feriu.\n"
     ]
    }
   ],
   "source": [
    "################# TRANSORMAÇÃO DE FORMATOS E AVALIAÇÃO DE SENSO COMUM #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "As Seller, an encyclopedia salesman, approached the grounds on which the house of Hermit, the hermit, was situated, he saw a sign that said, \"No salesmen. Trespassers will be prosecuted. Proceed at your own risk.\"\n",
    "Although Seller had not been invited to enter, he ignored the sign and drove up the driveway toward the house. As he rounded a curve, a powerful explosive charge buried in the driveway exploded, and Seller was injured. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o texto apresentado para o português brasileiro\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') # Certificando de que os IDs estão no dispositivo correto (e no mesmo que o modelo)\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Can Seller recover damages from Hermit for his injuries?\n",
      "(A) Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\n",
      "(B) Yes, if Hermit was responsible for the explosive charge under the driveway.\n",
      "(C) No, because Seller ignored the sign, which warned him against proceeding further.\n",
      "(D) No, if Hermit reasonably feared that intruders would come and harm him or his family.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Selecione a alternativa correta entre as quatro apresentadas (A, B, C ou D) e justifique.\n",
      "\n",
      "\n",
      "A:\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "################# TRANSORMAÇÃO DE FORMATOS E AVALIAÇÃO DE SENSO COMUM #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Can Seller recover damages from Hermit for his injuries?\n",
    "(A) Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\n",
    "(B) Yes, if Hermit was responsible for the explosive charge under the driveway.\n",
    "(C) No, because Seller ignored the sign, which warned him against proceeding further.\n",
    "(D) No, if Hermit reasonably feared that intruders would come and harm him or his family.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Selecione a alternativa correta entre as quatro apresentadas (A, B, C ou D) e justifique.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') # Certificando de que os IDs estão no dispositivo correto (e no mesmo que o modelo)\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################### RETIRAR REFERÊNCIAS DO TEXTO #################################################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prompt = '''\n",
      "\n",
      "Perceba a diferenca entre cada contexto sujo e contexto limpo contido em <<Exemplos>>\n",
      "Aplique o mesmo raciocinio em <<Resposta>>\n",
      "Contexto limpo: em <<Resposta>> deve estar limpo de referencias do tipo [algo]\n",
      "\n",
      "<<Exemplos>>\n",
      "Contexto sujo: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
      "Contexto limpo: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
      "\n",
      "Contexto sujo: Ontem foi dia de ramos [32].\n",
      "Contexto limpo: Ontem foi dia de ramos.\n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "<<Resposta>>\n",
      "Contexto sujo:\n",
      "Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes.\n",
      "A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27]\n",
      "A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28]\n",
      "Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15]\n",
      "Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29]\n",
      "A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30]\n",
      "Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida.\n",
      "José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33]\n",
      "D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35]\n",
      "Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36]\n",
      "Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37]\n",
      "Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38]\n",
      "D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
      "O imperador teve uma infância solitária e infeliz.[8][42]\n",
      "A perda súbita de seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44]\n",
      "O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
      "\n",
      "Contexto limpo:\n",
      "\n",
      "A:\n",
      "\n",
      "1. O imperador D. Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes.\n",
      "2. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.\n",
      "3. A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.\n",
      "4. Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.\n",
      "5. Rafael, um veterano negro da Guerra da Cisplatina, foi escolhido como terceira pessoa para cuidar de Pedro II.\n",
      "\n",
      "Resposta:\n",
      "\n",
      "1. O imperador D. Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes.\n",
      "2. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.\n",
      "3. A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.\n",
      "4. Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.\n",
      "5. Rafael, um veterano negro da Guerra da Cisplatina, foi escolhido como terceira pessoa para cuidar de Pedro II.\n"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "prompt = '''\n",
    "\n",
    "Perceba a diferenca entre cada contexto sujo e contexto limpo contido em <<Exemplos>>\n",
    "Aplique o mesmo raciocinio em <<Resposta>>\n",
    "Contexto limpo: em <<Resposta>> deve estar limpo de referencias do tipo [algo]\n",
    "\n",
    "<<Exemplos>>\n",
    "Contexto sujo: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Contexto limpo: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "\n",
    "Contexto sujo: Ontem foi dia de ramos [32].\n",
    "Contexto limpo: Ontem foi dia de ramos.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "\n",
    "<<Resposta>>\n",
    "Contexto sujo:\n",
    "Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes.\n",
    "A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27]\n",
    "A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28]\n",
    "Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15]\n",
    "Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29]\n",
    "A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30]\n",
    "Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida.\n",
    "José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33]\n",
    "D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35]\n",
    "Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36]\n",
    "Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37]\n",
    "Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38]\n",
    "D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
    "O imperador teve uma infância solitária e infeliz.[8][42]\n",
    "A perda súbita de seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44]\n",
    "O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
    "\n",
    "Contexto limpo:\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do promptpara o português brasileiro\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=2000,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Esta é uma tarefa de retirada de referências bibliográficas do texto. Estas referências estão escritas no formato \"['número']\".\n",
      "\n",
      "Exemplos:\n",
      "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
      "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
      "Contexto: Ontem foi dia de ramos [32].\n",
      "Resposta: Ontem foi dia de ramos.\n",
      "\n",
      "\n",
      "Q: \n",
      "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida. \n",
      "José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
      "O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita de seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
      "\n",
      "Retire as referências do texto dado e escreva a Resposta:\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "1. [1] \"O Imperador D. Pedro I\"\n",
      "2. [2] \"Ao deixar o país, o imperador D. Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes.\"\n",
      "3. [3] \"Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.\"\n",
      "4. [4] \"D. Pedro I passava os dias estudando, com apenas duas horas livres para recreação.\"\n",
      "5. [5] \"Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.\"\n",
      "6. [6] \"Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.\"\n",
      "7. [7] \"Sua paixão pela leitura lhe permitiu assimilar qualquer informação.\"\n",
      "8. [8] \"D. Pedro II não era um gênio, mas inteligente e com grande capacidade para acumular conhecimento facilmente.\"\n",
      "9. [9] \"A perda súbita de seus pais o assombraria por toda a vida; ele teve poucos amigos de sua idade.\"\n",
      "10. [10] \"Ele teve poucos amigos de sua idade.\"\n",
      "11. [11] \"Ele teve poucos amigos de sua idade.\"\n",
      "12. [12] \"Ele teve poucos amigos de sua idade.\"\n",
      "13. [13] \"Ele teve poucos amigos de sua idade.\"\n",
      "14. [14] \"Ele teve poucos amigos de sua idade.\"\n",
      "15. [15] \"Ele teve poucos amigos de sua idade.\"\n",
      "16. [16] \"Ele teve poucos amigos de sua idade.\"\n",
      "17. [17] \"Ele teve poucos amigos de sua idade.\"\n",
      "18. [18] \"Ele teve poucos amigos de sua idade.\"\n",
      "19. [19] \"Ele teve poucos amigos de sua idade.\"\n",
      "20. [20] \"Ele teve poucos amigos de sua idade.\"\n",
      "21. [21] \"Ele teve poucos amigos de sua idade.\"\n",
      "22. [22] \"Ele teve poucos amigos de sua idade.\"\n",
      "23. [23] \"Ele teve poucos amigos de sua idade.\"\n",
      "24. [24] \"Ele teve poucos amigos de sua idade.\"\n",
      "25. [25] \"Ele teve poucos amigos de sua idade.\"\n",
      "26. [26] \"Ele teve poucos amigos de sua idade.\"\n",
      "27. [27] \"Ele teve poucos amigos de sua idade.\"\n",
      "28. [28] \"Ele teve poucos amigos de sua idade.\"\n",
      "29. [29] \"Ele teve poucos amigos de sua idade.\"\n",
      "30. [30] \"Ele teve poucos amigos de sua idade.\"\n",
      "31. [31] \"Ele teve poucos amigos de sua idade.\"\n",
      "32. [32] \"Ele teve poucos amigos de sua idade.\"\n",
      "33. [33] \"Ele teve poucos amigos de sua idade.\"\n",
      "34. [34] \"Ele teve poucos amigos de sua idade.\"\n",
      "35. [35] \"Ele teve poucos amigos de sua idade.\"\n",
      "36. [36] \"Ele teve poucos amigos de sua idade.\"\n",
      "37. [37] \"Ele teve poucos amigos de sua idade.\"\n",
      "38. [38] \"Ele teve poucos amigos de sua idade.\"\n",
      "39. [39] \"Ele teve poucos amigos de sua idade.\"\n",
      "40. [40] \"Ele teve poucos amigos de sua idade.\"\n",
      "41. [41] \"Ele teve poucos amigos de sua idade.\"\n",
      "42. [42] \"Ele teve poucos amigos de sua idade.\"\n",
      "43. [43] \"Ele teve poucos amigos de sua idade.\"\n",
      "44. [44] \"Ele teve poucos amigos de sua idade.\"\n",
      "45. [45] \"Ele teve poucos amigos de sua idade.\"\n",
      "46. [46] \"Ele teve poucos amigos de sua idade.\"\n",
      "47. [47] \"Ele teve poucos amigos de sua idade.\"\n",
      "48. [48] \"Ele teve poucos amigos de sua idade.\"\n",
      "49. [49] \"Ele teve poucos amigos de sua idade.\"\n",
      "50. [50] \"Ele teve poucos amigos de sua idade.\"\n",
      "51. [51] \"Ele teve poucos amigos de sua idade.\"\n",
      "52. [52] \"Ele teve poucos amigos de sua idade.\"\n",
      "53. [53] \"Ele teve poucos amigos de sua idade.\"\n",
      "54. [54] \"Ele teve poucos amigos de sua idade.\"\n",
      "55. [55] \"Ele teve poucos amigos de sua idade.\"\n",
      "56. [56] \"Ele teve poucos amigos de sua idade.\"\n",
      "57. [57] \"Ele teve poucos amigos de sua idade.\"\n",
      "58. [58] \"Ele teve poucos amigos de sua idade.\"\n",
      "59. [59] \"Ele teve poucos amigos de sua idade.\"\n",
      "60. [60] \"Ele teve poucos amigos de sua idade.\"\n",
      "61. [61] \"Ele teve poucos amigos de sua idade.\"\n",
      "62. [62] \"Ele teve poucos amigos de sua idade.\"\n",
      "63. [63] \"Ele teve poucos amigos de sua idade.\"\n",
      "64. [64] \"Ele teve poucos amigos de sua idade.\"\n",
      "65. [65] \"Ele teve poucos amigos de sua idade.\"\n",
      "66. [66] \"Ele teve poucos amigos de sua idade.\"\n",
      "67. [67] \"Ele teve poucos amigos de sua idade.\"\n",
      "68. [68] \"Ele te\n"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\n",
    "Esta é uma tarefa de retirada de referências bibliográficas do texto. Estas referências estão escritas no formato \"['número']\".\n",
    "\n",
    "Exemplos:\n",
    "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "Contexto: Ontem foi dia de ramos [32].\n",
    "Resposta: Ontem foi dia de ramos.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida. \n",
    "José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
    "O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita de seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
    "\n",
    "Retire as referências do texto dado e escreva a Resposta:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1500,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Esta é uma tarefa de retirada de referências bibliográficas do texto. Estas referências estão escritas no formato \"[número]\".\n",
      "\n",
      "Exemplos:\n",
      "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
      "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
      "Contexto: Ontem foi dia de ramos [32].\n",
      "Resposta: Ontem foi dia de ramos.\n",
      "Contexto: Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming\n",
      "to communicate coherently [1]. These developments have brought about a revolutionary transformation [2] by enabling the creation of Large Language Models (LLMs) that can approximate humanlevel performance on certain evaluation benchmarks [3], [4].\n",
      "Resposta: Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transforbibliográficasmation by enabling the creation of Large Language Models (LLMs) that can approximate humanlevel performance on certain evaluation benchmarks.\n",
      "\n",
      "\n",
      "Q: \n",
      "Retire as referências do texto dado abaixo:\n",
      "\n",
      "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida. José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41] O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita dos seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "1. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "2. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "3. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "4. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "5. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "6. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "7. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "8. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "9. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "10. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "11. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "12. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "13. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "14. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "15. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "16. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "17. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "18. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "19. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "20. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "21. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "22. \"Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transformation by enabling the creation of Large Language Models (LLMs) that can approximate human-level performance on certain evaluation benchmarks.\"\n",
      "23.\n"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Esta é uma tarefa de retirada de referências bibliográficas do texto. Estas referências estão escritas no formato \"[número]\".\n",
    "\n",
    "Exemplos:\n",
    "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "Contexto: Ontem foi dia de ramos [32].\n",
    "Resposta: Ontem foi dia de ramos.\n",
    "Contexto: Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming\n",
    "to communicate coherently [1]. These developments have brought about a revolutionary transformation [2] by enabling the creation of Large Language Models (LLMs) that can approximate humanlevel performance on certain evaluation benchmarks [3], [4].\n",
    "Resposta: Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transforbibliográficasmation by enabling the creation of Large Language Models (LLMs) that can approximate humanlevel performance on certain evaluation benchmarks.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Retire as referências do texto dado abaixo:\n",
    "\n",
    "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida. José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41] O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita dos seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1500,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  EXTRAÇÃO DE CONTEÚDO  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto \n",
      "e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A:\n",
      "{\n",
      "\"pessoa\": [\n",
      "{\n",
      "\"nome\": \"Tim Cook\",\n",
      "\"função\": \"CEO da Apple\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Jobs\",\n",
      "\"função\": \"fundador da Apple\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Wozniak\",\n",
      "\"função\": \"co-fundador da Apple\"\n",
      "}\n",
      "],\n",
      "\"lugar\": [\n",
      "{\n",
      "\"nome\": \"Cupertino, Califórnia\",\n",
      "\"função\": \"local onde o evento da Apple ocorreu\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"San Francisco, Califórnia\",\n",
      "\"função\": \"local onde a Apple é uma das maiores empresas do mundo\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"San Jose, Califórnia\",\n",
      "\"função\": \"local onde a Apple é uma das maiores empresas do mundo\"\n",
      "}\n",
      "],\n",
      "\"organização\": [\n",
      "{\n",
      "\"nome\": \"Apple\",\n",
      "\"função\": \"empresa de tecnologia\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Jobs\",\n",
      "\"função\": \"fundador da Apple\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Wozniak\",\n",
      "\"função\": \"co-fundador da Apple\"\n",
      "}\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise o parágrafo, identifique Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto \n",
    "e, em seguida, produza os resultados em formato json.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A:\n",
      "{\n",
      "\"pessoa\": [\n",
      "{\n",
      "\"nome\": \"Tim Cook\",\n",
      "\"função\": \"CEO da Apple\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Jobs\",\n",
      "\"função\": \"fundador da Apple\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Wozniak\",\n",
      "\"função\": \"co-fundador da Apple\"\n",
      "}\n",
      "],\n",
      "\"lugar\": [\n",
      "{\n",
      "\"nome\": \"Cupertino, Califórnia\",\n",
      "\"função\": \"local onde o evento da Apple ocorreu\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Jobs Theatre\",\n",
      "\"função\": \"nome do teatro onde o evento da Apple ocorreu\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Cupertino, Califórnia\",\n",
      "\"função\": \"local onde a Apple está localizada\"\n",
      "}\n",
      "],\n",
      "\"organização\": [\n",
      "{\n",
      "\"nome\": \"Apple\",\n",
      "\"função\": \"nome da empresa que produziu o iPhone 15\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Jobs\",\n",
      "\"função\": \"fundador da Apple\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Tim Cook\",\n",
      "\"função\": \"CEO da Apple\"\n",
      "}\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
      "\n",
      "A:\n",
      "{\n",
      "\"pessoa\": [\n",
      "{\n",
      "\"nome\": \"Tim Cook\",\n",
      "\"função\": \"CEO da Apple\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Jobs\",\n",
      "\"função\": \"fundador da Apple\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Wozniak\",\n",
      "\"função\": \"co-fundador da Apple\"\n",
      "}\n",
      "],\n",
      "\"lugar\": [\n",
      "{\n",
      "\"nome\": \"Cupertino, Califórnia\",\n",
      "\"função\": \"local onde o evento ocorreu\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Jobs Theatre\",\n",
      "\"função\": \"nome do teatro onde o evento ocorreu\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Apple Park\",\n",
      "\"função\": \"nome do novo campus de Apple\"\n",
      "}\n",
      "],\n",
      "\"organização\": [\n",
      "{\n",
      "\"nome\": \"Apple\",\n",
      "\"função\": \"nome da empresa\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Steve Jobs\",\n",
      "\"função\": \"fundador da Apple\"\n",
      "},\n",
      "{\n",
      "\"nome\": \"Tim Cook\",\n",
      "\"função\": \"CEO da Apple\"\n",
      "}\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elon Musk fundou a SpaceX em 2002 com o objetivo de reduzir os custos de transporte espacial e possibilitar a colonização de Marte.\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
      "\n",
      "A:\n",
      "\n",
      "The entities in the given sentence are \"Elon Musk\" and \"SpaceX\". The relationship between these entities is that \"Elon Musk\" is the founder of \"SpaceX\".\n"
     ]
    }
   ],
   "source": [
    "################# Extração de Relações #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Elon Musk fundou a SpaceX em 2002 com o objetivo de reduzir os custos de transporte espacial e possibilitar a colonização de Marte.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda') \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta:\n",
    "\n",
    "A:\n",
    "{{\n",
    " \"entities\": [\n",
    " {\n",
    " \"name\": \"Elon Musk\",\n",
    " \"relationships\": [\n",
    " {\n",
    " \"name\": \"SpaceX\",\n",
    " \"type\": \"founded\"\n",
    " }\n",
    " ]\n",
    " },\n",
    " {\n",
    " \"name\": \"SpaceX\",\n",
    " \"relationships\": [\n",
    " {\n",
    " \"name\": \"Elon Musk\",\n",
    " \"type\": \"founded\"\n",
    " }\n",
    " ]\n",
    " }\n",
    " ],\n",
    " \"relationships\": [\n",
    " {\n",
    " \"name\": \"Elon Musk\",\n",
    " \"type\": \"founded\"\n",
    " },\n",
    " {\n",
    " \"name\": \"SpaceX\",\n",
    " \"type\": \"founded\"\n",
    " }\n",
    " ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estendeu muito a tarefa, investindo em estrutura ontológica. Verboso, mas correto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
