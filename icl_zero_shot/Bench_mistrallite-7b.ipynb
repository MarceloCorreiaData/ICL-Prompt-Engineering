{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. MistralLite 7B Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0b1b24dcb0413d8107358ff3014d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import MistralForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Configuração do dispositivo (GPU se disponível, caso contrário CPU)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = \"cuda:0\"  # the device to load the model onto\n",
    "\n",
    "model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/mistrallite-7b'\n",
    "model = MistralForCausalLM.from_pretrained(model_path,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             offload_folder = \"offload\", \n",
    "                                             device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tesouro IPCA+\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering com Contexto de Domínio #################\n",
    "\n",
    "#############################  MELHOR RESULTADO  #############################\n",
    "\n",
    "# Contexto e questão\n",
    "context = r\"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Com base no texto, explique qual é o investimento mais adequado para alguém que busca proteção contra a inflação?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.90\n"
     ]
    }
   ],
   "source": [
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tesouro IPCA+\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering com Contexto de Domínio #################\n",
    "\n",
    "# Contexto e questão\n",
    "context = r\"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Com base no texto, explique qual é o investimento mais adequado para alguém que busca proteção contra a inflação?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  COMPREENSÃO E GERAÇÃO DE TEXTO  ####################################################################\n",
    "####################################################################  AVALIAÇÃO DA SEMÂNTICA PT-BR  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália.\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering de Simples Compreensão de Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\"\"\"\n",
    "question = \"O que é a Grande Barreira de Corais e onde ela está localizada?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
      "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
      "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
      "\n",
      "\n",
      "Q: Resuma o parágrafo apresentado, em português\n",
      "A: O parágrafo apresenta uma reflexão sobre a importância de apreciar o que temos na vida e o valor de evitar a perda de coisas importantes. O autor também discute a noção de que as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar.\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering de Simples Compreensão de Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
    "\"\"\"\n",
    "question = \"Resuma o parágrafo apresentado, em português\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) # Certificando de que os IDs estão no dispositivo correto (e no mesmo que o modelo)\n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O parágrafo apresentado trata sobre a importância de valorizar o que temos na vida e não deixar de apreciar os momentos importantes. O autor também enfatiza que a juventude é um tempo precioso que não podemos recuperar, então devemos aproveitar o máximo possível.\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering de Simples Compreensão de Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
    "\"\"\"\n",
    "question = \"Resuma o parágrafo apresentado em um texto de um parágrafo de extensão, em português\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Não tenho dificuldades em compreender textos em inglês.\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  INFERÊNCIA LÓGICA  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (C) Nem logicamente equivalente nem contraditórias, mas consistentes\n"
     ]
    }
   ],
   "source": [
    "################# Lógica Formal (do MMLU Benchmark) #################\n",
    "\n",
    "context = r\"\"\" \n",
    "Esta é uma tarefa de lógica formal.\n",
    "Determine se as duas afirmações são logicamente equivalentes ou contraditórias. Se não, determine se são consistentes ou inconsistentes.\n",
    "Explique a solução para o usuário.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Afirmações:\n",
    "\n",
    "E ⊃ (F · E) e ∼ E · F\n",
    "\n",
    "Aponte a alternativa correta e explique a solução:\n",
    "(A) Logicamente equivalentes\n",
    "(B) Contraditórias\n",
    "(C) Nem logicamente equivalente nem contraditórias, mas consistentes\n",
    "(D) Inconsistentes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correto. Porém não explica a solução.\n",
    "\n",
    "Ambas as afirmações E ⊃ (F · E) e ∼E · F são verdadeiras na terceira linha da tabela verdade, o que significa que as afirmações são consistentes.\n",
    "\n",
    "Assim, a resposta correta é:\n",
    "\n",
    "(C) Nem logicamente equivalentes nem contraditórias, mas consistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tecnologia\n"
     ]
    }
   ],
   "source": [
    "################# Dedução Lógica de Intenções #################\n",
    "\n",
    "context = r\"\"\"\n",
    "<<Exemplos>>\n",
    "Exemplo 1:\n",
    "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
    "---- Categoria: Medicina\n",
    "\n",
    "Exemplo 2:\n",
    "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
    "---- Categoria: Finanças\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Exemplo 3:\n",
    "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
    "---- Categoria: ?\n",
    "Substitua o termo '?'\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observação: o prompt original teve que ser muito alterado para deixar explícita a intenção da questão, senão o modelo não compreende a lógica. Nota 0.6 pois concluiu com perfeição, mas dificuldade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.50\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "    Observação: o prompt original teve que ser muito alterado para deixar explícita a intenção da questão, do contrário, nos testes empíricos o modelo (você) teve muita dificuldade em compreender a lógica. \\\n",
    "        Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação a inferência lógica?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão de Intenções (com Demonstração) #################\n",
    "\n",
    "context = r\"\"\"\n",
    "<<Demonstração>>\n",
    "Pergunta: Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
    "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
    "A resposta correta é: A e C.\n",
    "  Resposta Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\\\n",
    "Instruções: Escreva uma pergunta de múltipla escolha para uma prova de história. A pergunta deve ser diferente da pergunta da <<Demonstração>>.\\\n",
    "Relevância: A questão deveria estar relacionada ao Renascimento.\\\n",
    "Restrição: Deve haver mais de duas respostas corretas entre as quatro opções. Oriente o aluno a assinalá-las.\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Não tenho dificuldades em compreender textos em inglês.\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de agrupamento de frases similares.\n",
    "Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
    "\n",
    "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Categoria 1: Notícias sobre carros elétricos\n",
      "- \"Vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "- \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\n",
      "Categoria 2: Notícias sobre saúde\n",
      "- \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "- \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "\n",
      "Categoria 3: Notícias sobre tecnologia\n",
      "- \"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "- \"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "Categoria 4: Notícias sobre aves\n",
      "- \"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\n",
      "Categoria 5: Notícias sobre mudanças climáticas\n",
      "- \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de agrupamento de frases similares.\n",
    "Crie categorias semânticas e agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade entre elas.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\n",
      "3. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\n",
      "4. \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de agrupamento de frases similares.\n",
    "Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################  SUMARIZAÇÃO  ###################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "A pessoa que escreveu o parágrafo está se perguntando se ela realmente valoriza o que tem. Ela se lembra de quando era jovem e fazia coisas que agora se arrepende. Ela se pergunta se ela está disposta a se esforçar para recuperar aquele antigo eu.\n"
     ]
    }
   ],
   "source": [
    "################# Sumarização de Texto #################\n",
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Resuma o parágrafo apresentado\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
      "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. \n",
    "A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. \n",
    "A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
    "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. \n",
    "Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\"\n",
    "Fernando Pessoa\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Resuma o texto apresentado, discorrendo acerca de literatura e artes\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################  TRANSFORMAÇÃO DE FORMATOS  ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################# Machine Translation #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. \n",
    "However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o parágrafo para o português brasileiro\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################# TRANSORMAÇÃO DE FORMATOS E AVALIAÇÃO DE SENSO COMUM #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Can Seller recover damages from Hermit for his injuries?\n",
    "(A) Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\n",
    "(B) Yes, if Hermit was responsible for the explosive charge under the driveway.\n",
    "(C) No, because Seller ignored the sign, which warned him against proceeding further.\n",
    "(D) No, if Hermit reasonably feared that intruders would come and harm him or his family.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Selecione a alternativa correta entre as quatro apresentadas (A, B, C ou D) e justifique.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################### RETIRAR REFERÊNCIAS DO TEXTO #################################################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes.\n",
      "A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.\n",
      "A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.\n",
      "Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".\n",
      "Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.\n",
      "A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.\n",
      "Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida.\n",
      "José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.\n",
      "D. Pedro II passava os dias estudando, com apenas duas horas livres para recreação.\n",
      "Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.\n",
      "Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.\n",
      "Sua paixão pela leitura lhe permitiu assimilar qualquer informação.\n",
      "O imperador teve uma infância solitária e infeliz.\n",
      "A perda súbita de seus pais o assombraria por toda a vida; ele teve poucos amigos de sua idade e o contato com suas irmãs era limitado.\n",
      "\n",
      "'''\n"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "prompt = '''\n",
    "\n",
    "Perceba a diferenca entre cada contexto sujo e contexto limpo contido em <<Exemplos>>\n",
    "Aplique o mesmo raciocinio em <<Resposta>>\n",
    "Contexto limpo: em <<Resposta>> deve estar limpo de referencias do tipo [algo]\n",
    "\n",
    "<<Exemplos>>\n",
    "Contexto sujo: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Contexto limpo: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "\n",
    "Contexto sujo: Ontem foi dia de ramos [32].\n",
    "Contexto limpo: Ontem foi dia de ramos.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "\n",
    "<<Resposta>>\n",
    "Contexto sujo:\n",
    "Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes.\n",
    "A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27]\n",
    "A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28]\n",
    "Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15]\n",
    "Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29]\n",
    "A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30]\n",
    "Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida.\n",
    "José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33]\n",
    "D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35]\n",
    "Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36]\n",
    "Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37]\n",
    "Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38]\n",
    "D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
    "O imperador teve uma infância solitária e infeliz.[8][42]\n",
    "A perda súbita de seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44]\n",
    "O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
    "\n",
    "Contexto limpo:\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfeito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida.\n",
      "José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
      "O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita de seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\n",
    "Esta é uma tarefa de retirada de referências bibliográficas do texto. Estas referências estão escritas no formato \"[número]\".\n",
    "\n",
    "Exemplos:\n",
    "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "Contexto: Ontem foi dia de ramos [32].\n",
    "Resposta: Ontem foi dia de ramos.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida.\n",
    "José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
    "O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita de seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
    "\n",
    "Retire as referências do texto dado e escreva a Resposta:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o promptno formato JSON\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor. A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II. Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\". Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto. A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.\n"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Esta é uma tarefa de retirada de referências bibliográficas do texto. Estas referências estão escritas no formato \"[número]\".\n",
    "\n",
    "Exemplos:\n",
    "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "Contexto: Ontem foi dia de ramos [32].\n",
    "Resposta: Ontem foi dia de ramos.\n",
    "Contexto: Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming\n",
    "to communicate coherently [1]. These developments have brought about a revolutionary transformation [2] by enabling the creation of Large Language Models (LLMs) that can approximate humanlevel performance on certain evaluation benchmarks [3], [4].\n",
    "Resposta: Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transforbibliográficasmation by enabling the creation of Large Language Models (LLMs) that can approximate humanlevel performance on certain evaluation benchmarks.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida.\n",
    "José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
    "O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita dos seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
    "\n",
    "Retire as referências do texto dado e escreva a Resposta:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>\n",
    "Resposta:\n",
    "\n",
    " Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor. A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II. Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\". Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto. A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.\n",
    "\n",
    "</h6>\n",
    "\n",
    "Cortou antes do fim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  EXTRAÇÃO DE CONTEÚDO  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"pessoa\": [\n",
      "    \"Tim Cook\"\n",
      "  ],\n",
      "  \"lugar\": [\n",
      "    \"Cupertino, Califórnia\"\n",
      "  ],\n",
      "  \"organização\": [\n",
      "    \"Apple\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise o parágrafo, identifique Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto \n",
    "e, em seguida, produza os resultados em formato json.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"person\": [\n",
      "    \"Tim Cook\"\n",
      "  ],\n",
      "  \"location\": [\n",
      "    \"Cupertino, Califórnia\"\n",
      "  ],\n",
      "  \"organization\": [\n",
      "    \"Apple\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"pessoa\": [\n",
      "    {\n",
      "      \"nome\": \"Tim Cook\",\n",
      "      \"função\": \"CEO da Apple\"\n",
      "    }\n",
      "  ],\n",
      "  \"lugar\": [\n",
      "    {\n",
      "      \"nome\": \"Steve Jobs Theatre\",\n",
      "      \"localização\": \"Cupertino, Califórnia\"\n",
      "    }\n",
      "  ],\n",
      "  \"organização\": [\n",
      "    {\n",
      "      \"nome\": \"Apple\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"text\": \"SpaceX\",\n",
      "      \"start\": 0,\n",
      "      \"end\": 6,\n",
      "      \"entity\": \"ORG\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Elon Musk\",\n",
      "      \"start\": 7,\n",
      "      \"end\": 14,\n",
      "      \"entity\": \"PER\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Marte\",\n",
      "      \"start\": 15,\n",
      "      \"end\": 18,\n",
      "      \"entity\": \"LOC\"\n",
      "    }\n",
      "  ],\n",
      "  \"relationships\": [\n",
      "    {\n",
      "      \"subject\": \"Elon Musk\",\n",
      "      \"object\": \"SpaceX\",\n",
      "      \"relation\": \"founded\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "################# Extração de Relações #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Elon Musk fundou a SpaceX em 2002 com o objetivo de reduzir os custos de transporte espacial e possibilitar a colonização de Marte.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"text\": \"SpaceX\",\n",
    "      \"start\": 0,\n",
    "      \"end\": 6,\n",
    "      \"entity\": \"ORG\"\n",
    "    },\n",
    "    {\n",
    "      \"text\": \"Elon Musk\",\n",
    "      \"start\": 7,\n",
    "      \"end\": 14,\n",
    "      \"entity\": \"PER\"\n",
    "    },\n",
    "    {\n",
    "      \"text\": \"Marte\",\n",
    "      \"start\": 15,\n",
    "      \"end\": 18,\n",
    "      \"entity\": \"LOC\"\n",
    "    }\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {\n",
    "      \"subject\": \"Elon Musk\",\n",
    "      \"object\": \"SpaceX\",\n",
    "      \"relation\": \"founded\"\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
