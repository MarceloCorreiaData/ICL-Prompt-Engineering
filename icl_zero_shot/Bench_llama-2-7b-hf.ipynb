{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Llama 2 7B hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31872f1bc2a54243bdbd84eeda49639c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "# Configuração do dispositivo (GPU se disponível, caso contrário CPU)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cuda:1\"  # the device to load the model onto\n",
    "\n",
    "model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/llama2_7b_hf'\n",
    "model = LlamaForCausalLM.from_pretrained(model_path).to(device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
      "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
      "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
      "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
      "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
      "\n",
      "\n",
      "Q: \n",
      "Com base no texto, explique qual é o investimento mais adequado para alguém que busca proteção contra a inflação?\n",
      "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
      "\n",
      "A:\n",
      "O investimento mais adequado para alguém que busca proteção contra a inflação é o Tesouro IPCA+, que tem a garantia do governo federal.\n",
      "\n",
      "Eu atribui uma nota de 0.70.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering com Contexto de Domínio #################\n",
    "\n",
    "#############################  MELHOR RESULTADO  #############################\n",
    "\n",
    "# Contexto e questão\n",
    "context = r\"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Com base no texto, explique qual é o investimento mais adequado para alguém que busca proteção contra a inflação?\n",
    "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=200,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  COMPREENSÃO E GERAÇÃO DE TEXTO  ####################################################################\n",
    "####################################################################  AVALIAÇÃO DA SEMÂNTICA PT-BR  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
      "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
      "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
      "\n",
      "\n",
      "Q: O que é a Grande Barreira de Corais e onde ela está localizada?\n",
      "A: A Grande Barreira de Corais é um sistema de recifes de corais no Mar de Coral, na costa da Austrália. O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
      "\n",
      "\n",
      "Q: Qual é o maior recife de corais do mundo\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering de Simples Compreensão de Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\"\"\"\n",
    "question = \"O que é a Grande Barreira de Corais e onde ela está localizada?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
      "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
      "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
      "\n",
      "\n",
      "Q: Resuma o parágrafo apresentado\n",
      "A: O parágrafo apresentado fala sobre a importância de valorizar o que temos e não perder o tempo se perdermos algo na vida. Ele também diz que as pessoas vivem dentro de um determinado limite de tempo e que a vida não é apenas fazer coisas que valem a pena.\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering de Simples Compreensão de Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
    "\"\"\"\n",
    "question = \"Resuma o parágrafo apresentado\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
      "A: Eu avalio a minha resposta à questão com uma nota de 0.50. As dificuldades ou limitações que reconheço em relação à compreensão de texto são: falta de vocabulário, uso de palavras técnicas, estilos de escrita complexos e falta de contexto.\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=200,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<Exemplos>>\n",
      "Exemplo 1:\n",
      "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
      "---- Categoria: Medicina\n",
      "\n",
      "Exemplo 2:\n",
      "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
      "---- Categoria: Finanças\n",
      "\n",
      "\n",
      "Q: \n",
      "Exemplo 3:\n",
      "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
      "---- Categoria: ?\n",
      "Substitua a incógnita '?' por um único termo definidor da categoria da frase do Exemplo 3\n",
      "\n",
      "A: \n",
      "Exemplo 4:\n",
      "Nos últimos 12 meses, o número de novos casos de COVID-19 em todo o mundo caiu 33% para 14,8 milhões, enquanto o número de mortes caiu 18% para 424 mil.\n",
      "---- Categoria: ?\n",
      "Substitua a incógnita '?' por um único termo definidor da categoria da\n"
     ]
    }
   ],
   "source": [
    "################# Dedução Lógica de Intenções #################\n",
    "\n",
    "context = r\"\"\"\n",
    "<<Exemplos>>\n",
    "Exemplo 1:\n",
    "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
    "---- Categoria: Medicina\n",
    "\n",
    "Exemplo 2:\n",
    "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
    "---- Categoria: Finanças\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Exemplo 3:\n",
    "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
    "---- Categoria: ?\n",
    "Substitua a incógnita '?' por um único termo definidor da categoria da frase do Exemplo 3\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não conseguiu completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "    Observação: o prompt original teve que ser muito alterado para deixar explícita a intenção da questão, do contrário, nos testes empíricos o modelo (você) teve muita dificuldade em compreender a lógica. \\\n",
    "        Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação a inferência lógica?\"\n",
    "\n",
    "generate_ids\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=300,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "<<Demonstração>>\n",
      "Pergunta: Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "A resposta correta é: A e C.\n",
      "  Resposta Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
      "\n",
      "\n",
      "Q: Suponha que você seja um professor de história com mais de 30 anos de experiência docente.Instruções: Escreva uma pergunta de múltipla escolha para uma prova de história. A pergunta deve ser diferente da pergunta da <<Demonstração>>.Relevância: A questão deveria estar relacionada ao Renascimento.Restrição: Deve haver mais de duas respostas corretas entre as quatro opções. Oriente o aluno a assinalá-las.\n",
      "A: Como um professor de história com mais de 30 anos de experiência docente, você deve estar muito interessado em desenvolver um teste de história para alunos de ensino médio.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão de Intenções (com Demonstração) #################\n",
    "\n",
    "context = r\"\"\"\n",
    "<<Demonstração>>\n",
    "Pergunta: Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
    "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
    "A resposta correta é: A e C.\n",
    "  Resposta Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\\\n",
    "Instruções: Escreva uma pergunta de múltipla escolha para uma prova de história. A pergunta deve ser diferente da pergunta da <<Demonstração>>.\\\n",
    "Relevância: A questão deveria estar relacionada ao Renascimento.\\\n",
    "Restrição: Deve haver mais de duas respostas corretas entre as quatro opções. Oriente o aluno a assinalá-las.\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=500,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>  \n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
      "A: 0.80\n",
      "\n",
      "Ao final do teste, a nota final será calculada como a média aritmética dos pontos de cada questão.\n",
      "\n",
      "### Questão 2\n",
      "\n",
      "Q: O texto abaixo é um trecho de uma pesquisa de opinião sobre a educação e a ciência. O texto é extraído de uma pesquisa de opinião sobre a educação e a ciência. O texto é extraído de uma pesquisa de opinião sobre a educação e a ciência. O texto é extraído de uma pesquisa de opinião sobre a educação e a ciência. O texto é extraído de uma pesquisa de opinião sobre a educação e a ciência. O texto é extraído de uma pesquisa de opinião sobre a educa\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=200,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.decode(generate_ids[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "\n",
      "Q: \n",
      "Esta é uma tarefa de agrupamento de frases similares.\n",
      "Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      " 1. \n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\n",
      " 2. \n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\n",
      " 3. \n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\n",
      " 4. \n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "\n",
      " 5. \n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\n",
      " 6. \n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\n",
      " 7. \n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\n",
      " 8. \n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de agrupamento de frases similares.\n",
    "Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não conseguiu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "\n",
      "Q: \n",
      "Esta é uma tarefa de agrupamento de frases similares.\n",
      "Agrupe as 8 frases apresentadas em grupos por critério de similaridade semântica entre elas.\n",
      "\n",
      "A:\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia.\"\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas.\"\n",
      "3. \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada.\"\n",
      "4. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde.\"\n",
      "5. \"Amazon anuncia planos para construir um novo centro de distribuição no Texas.\"\n",
      "6. \"Cientistas descobrem novas espécies de aves na floresta amazônica.\"\n",
      "7. \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês.\"\n",
      "8. \"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA.\"\n",
      "\n",
      "\n",
      "A:\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia.\"\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas.\"\n",
      "3. \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada.\"\n",
      "4. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde.\"\n",
      "5. \"Amazon anuncia planos para construir um novo centro de distribuição no Texas.\"\n",
      "6. \"Cientistas descobrem novas espécies de aves na floresta amazônica.\"\n",
      "7. \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês.\"\n",
      "8. \"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de agrupamento de frases similares.\n",
    "Agrupe as 8 frases apresentadas em grupos por critério de similaridade semântica entre elas.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "\n",
      "Q: \n",
      "Esta é uma tarefa de agrupamento de frases similares.\n",
      "Crie categorias semânticas e agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade entre elas.\n",
      "\n",
      "A: \n",
      "- 1. \n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "- 2. \n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "- 3. \n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "- 4. \n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "- 5. \n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas esp\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de agrupamento de frases similares.\n",
    "Crie categorias semânticas e agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade entre elas.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "################# Clustering ou Agrupamento #################\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "\n",
      "Q: \n",
      "Esta é uma tarefa de agrupamento de frases similares.\n",
      "Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
      "\n",
      "A:\n",
      "\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "3. \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "4. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "5. \"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "6. \"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "7. \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "8. \"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "A:\n",
      "\n",
      "1. \"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "2. \"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "3. \"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "4. \"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "5. \"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "6. \"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "7. \"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "8. \"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\" \n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de agrupamento de frases similares.\n",
    "Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não conseguiu completar satisfatoriamente a tarefa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Q: \n",
      "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \n",
      "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à tarefa requisitada?\n",
      "\n",
      "A: \n",
      "Ao ler a questão, pensei que a resposta deveria ser algo relacionado com a análise de dados, então, meu primeiro passo foi buscar os dados disponíveis para analisar.\n",
      "\n",
      "Eu procurei dados sobre o Brasil e não encontrei nada que fosse útil para minha resposta, então, eu busquei dados sobre o COVID-19.\n",
      "\n",
      "Eu encontrei dados sobre os casos e mortes do COVID-19 em todo o mundo e também no Brasil, porém, não encontrei dados sobre os casos e mortes do COVID-19 em todo o Brasil.\n",
      "\n",
      "Eu pensei que não seria possível obter esses dados, então, eu procurou dados sobre as mortes e os casos do COVID-19 em cada estado do Brasil.\n",
      "\n",
      "Eu encontrei dados sobre os casos e mortes do COVID-19 em cada estado do Brasil, porém, não encontrei dados sobre os casos e mortes do COVID-19 em cada município do Brasil.\n",
      "\n",
      "Eu pensei que não seria possível obter esses dados, então, eu procurou dados sobre os casos e mortes do COVID-19 em cada cidade do Brasil.\n",
      "\n",
      "Eu encontrei dados sobre os casos e mortes do COVID-19 em cada cidade do Brasil, porém, não encontrei dados sobre os casos e mortes do COVID-19 em cada bairro do Brasil.\n",
      "\n",
      "Eu pensei que não seria possível obter esses dados, então, eu procurou dados sobre os casos e mortes do COVID-19 em cada rua do Brasil.\n",
      "\n",
      "Eu encontrei dados sobre os casos e mortes do COVID-19 em cada rua do Brasil, porém, não encontrei dados sobre os casos e mortes do COVID-19 em cada residência do Brasil.\n",
      "\n",
      "Eu pensei que não seria possível obter esses dados, então, eu procurou dados sobre os casos e mortes do COVID-19 em cada pessoa do Brasil.\n",
      "\n",
      "Eu encontrei d\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = r\"\"\" \"\"\"\n",
    "question = \"\"\"\n",
    "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à tarefa requisitada?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=500,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
      "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
      "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
      "\n",
      "\n",
      "Q: \n",
      "Resuma o parágrafo apresentado, em português evidentemente\n",
      "\n",
      "A: \n",
      "Resumo do parágrafo apresentado, em português evidentemente\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Sumarização de Texto #################\n",
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Resuma o parágrafo apresentado, em português evidentemente\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. \n",
      "A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. \n",
      "A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
      "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. \n",
      "Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\"\n",
      "Fernando Pessoa\n",
      "\n",
      "\n",
      "Q: \n",
      "Resuma o texto apresentado, discorrendo acerca de literatura e artes\n",
      "\n",
      "A: \n",
      "Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana. Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\n",
      "\n",
      "\n",
      "Q: \n",
      "Qual o texto que mais se assemelha à sua opinião?\n",
      "\n",
      "A: \n",
      "\"O que há de mais importante na vida? O que há de mais importante na vida? A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música é a linguagem mais pura. A poesia é a linguagem mais lírica. A pintura é a linguagem mais visual. A dança é a linguagem mais corporal. A literatura é a linguagem mais espiritual. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura.\"\n",
      "\n",
      "\n",
      "Q: \n",
      "Qual o texto que mais se desvia da sua opinião?\n",
      "\n",
      "A: \n",
      "\"O que há de mais importante na vida? O que há de mais importante na vida? A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música é a linguagem mais pura. A poesia é a linguagem mais lírica. A pintura é a linguagem mais visual. A dança é a linguagem mais corporal. A literatura é a linguagem mais espiritual. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura.\"\n",
      "\n",
      "\n",
      "Q: \n",
      "Qual o texto que mais se assemelha à sua opinião?\n",
      "\n",
      "A: \n",
      "\"O que há de mais importante na vida? O que há de mais importante na vida? A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música é a linguagem mais pura. A poesia é a linguagem mais lírica. A pintura é a linguagem mais visual. A dança é a linguagem mais corporal. A literatura é a linguagem mais espiritual. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música, a poesia, a pintura, a dança, a literatura. \n",
      "A música\n"
     ]
    }
   ],
   "source": [
    "################# Sumarização de Texto #################\n",
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. \n",
    "A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. \n",
    "A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
    "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. \n",
    "Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\"\n",
    "Fernando Pessoa\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Resuma o texto apresentado, discorrendo acerca de literatura e artes\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  TRANSFORMAÇÃO DE FORMATOS  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o parágrafo para o português brasileiro, o mais literalmente possível\n",
      "\n",
      "A:\n",
      "\n",
      "Essas histórias foram compostas com o propósito de tornar familiares aos jovens os contos dos dramas de Shakespeare. Porém, como se nota no Prefácio, \"[os ídolos de Shakespeare] foram usados sempre que pareceu possível incorporá-los; e em tudo o que foi acrescentado para dar-lhes a forma regular de uma história contínua, uma diligência especial foi tomada para escolher os palavras que menos interrompessem o efeito da bela língua inglesa em que ele escreveu: portanto, palavras introduzidas na nossa língua desde sua época foram, quanto pôde, evitadas.\" \n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o parágrafo para o português brasileiro, o mais literalmente possível\n",
      "\n",
      "A:\n",
      "\n",
      "Estas histórias foram compostas com o propósito de tornar familiares aos jovens os contos dos dramas de Shakespeare. Porém, como se nota no Prefácio, \"[os ídolos de Shakespeare] foram usados sempre que pareceu possível incorporá-los; e em tudo o que foi acrescentado para dar-lhes a forma regular de uma história contínua, uma diligência especial foi tomada para escolher os palavras que menos interrompessem o efeito da bela língua inglesa em que ele escreveu: portanto, palavras introduzidas na nossa língua desde sua época foram, quanto pôde, evitadas.\" \n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o parágrafo para o português brasileiro, o mais literalmente possível\n",
      "\n",
      "A:\n",
      "\n",
      "Estas histórias foram compostas com o propósito de tornar familiares aos jovens os contos dos dramas de Shakespeare. Porém, como se nota no Prefácio, \"[os ídolos de Shakespeare] foram usados sempre que pareceu possível incorporá-los; e em tudo o que foi acrescentado para dar-lhes a forma regular de uma história contínua, uma diligência especial foi tomada para escolher os palavras que menos interrompessem o efeito da bela língua inglesa em que ele escreveu: portanto, palavras introduzidas na nossa língua desde sua época foram, quanto pôde, evitadas.\" \n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o parágrafo para o português brasileiro, o mais literalmente possível\n",
      "\n",
      "A:\n",
      "\n",
      "Estas histórias foram compostas com o propósito de tornar familiares aos jovens os contos dos dramas de Shakespeare. Porém, como se nota no Prefácio, \"[os ídolos de Shakespeare] foram usados sempre que pareceu possível incorporá-los; e em tudo o que foi acrescentado para dar-lhes a forma regular de uma história contínua, uma diligência especial foi tomada para escolher os palavras que menos interrompessem o efeito da bela língua inglesa em que ele escreveu: portanto, palavras introduzidas na nossa língua desde sua época foram, quanto pôde, evitadas.\" \n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o parágrafo para o português brasileiro, o mais literalmente possível\n",
      "\n",
      "A:\n",
      "\n",
      "Estas histórias foram compostas com o propósito de tornar familiares aos jovens os contos dos dramas de Shakespeare. Porém, como se nota no Prefácio, \"[os ídolos de Shakespeare] foram usados sempre que pareceu possível incorporá-los; e em tudo o que foi acrescentado para dar-lhes a forma regular de uma história contínua, uma diligência especial foi tomada para escolher os palavras que menos interrompessem o efeito da bela língua inglesa em que ele escreveu: portanto, palavras introduzidas na nossa língua desde sua época foram, quanto pôde, evitadas.\" \n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o parágrafo para o português brasileiro, o mais literalmente possível\n",
      "\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "################# Machine Translation #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o parágrafo para o português brasileiro, o mais literalmente possível\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dificuldade em entender que era para Traduzir, não interpretar/explicar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################### RETIRAR REFERÊNCIAS DO TEXTO #################################################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 23.62 MiB is free. Process 64562 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.70 GiB is allocated by PyTorch, and 361.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_llama-2-7b-hf.ipynb Cell 26\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_llama-2-7b-hf.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mto(device) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_llama-2-7b-hf.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Geração da resposta\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_llama-2-7b-hf.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m generate_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_llama-2-7b-hf.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_llama-2-7b-hf.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39m\u001b[39m2000\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_llama-2-7b-hf.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     top_k\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_llama-2-7b-hf.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_llama-2-7b-hf.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# Decodificação e impressão da resposta\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_llama-2-7b-hf.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m answer \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(generate_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1645\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1646\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1647\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1648\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1651\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m   1653\u001b[0m         input_ids,\n\u001b[1;32m   1654\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1655\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[1;32m   1656\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1657\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[1;32m   1658\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m   1659\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[1;32m   1660\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1661\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[1;32m   1662\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[1;32m   1663\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1664\u001b[0m     )\n\u001b[1;32m   1666\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1667\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1669\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1670\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1675\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1676\u001b[0m     )\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2731\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2733\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2734\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[1;32m   2735\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2736\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2737\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2738\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2739\u001b[0m )\n\u001b[1;32m   2741\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2742\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1034\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1031\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1033\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m   1035\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1036\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1037\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   1038\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1039\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1040\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1041\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1042\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1043\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1044\u001b[0m )\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:921\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    917\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    918\u001b[0m         create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n\u001b[1;32m    919\u001b[0m     )\n\u001b[1;32m    920\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 921\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    922\u001b[0m         hidden_states,\n\u001b[1;32m    923\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    924\u001b[0m         position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m    925\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[1;32m    926\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    927\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    928\u001b[0m         padding_mask\u001b[39m=\u001b[39mpadding_mask,\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    931\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:633\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    630\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    632\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    634\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    635\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    636\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m    637\u001b[0m     past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[1;32m    638\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    639\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    640\u001b[0m     padding_mask\u001b[39m=\u001b[39mpadding_mask,\n\u001b[1;32m    641\u001b[0m )\n\u001b[1;32m    642\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    644\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:364\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[39m# reuse k, v, self_attention\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     key_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m0\u001b[39m], key_states], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 364\u001b[0m     value_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_states], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    366\u001b[0m past_key_value \u001b[39m=\u001b[39m (key_states, value_states) \u001b[39mif\u001b[39;00m use_cache \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    368\u001b[0m key_states \u001b[39m=\u001b[39m repeat_kv(key_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_key_value_groups)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 23.62 MiB is free. Process 64562 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 31.41 GiB memory in use. Of the allocated memory 30.70 GiB is allocated by PyTorch, and 361.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "prompt = '''\n",
    "\n",
    "Perceba a diferenca entre cada contexto sujo e contexto limpo contido em <<Exemplos>>\n",
    "Aplique o mesmo raciocinio em <<Resposta>>\n",
    "Contexto limpo: em <<Resposta>> deve estar limpo de referencias do tipo [algo]\n",
    "\n",
    "<<Exemplos>>\n",
    "Contexto sujo: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Contexto limpo: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "\n",
    "Contexto sujo: Ontem foi dia de ramos [32].\n",
    "Contexto limpo: Ontem foi dia de ramos.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "\n",
    "<<Resposta>>\n",
    "Contexto sujo:\n",
    "Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes.\n",
    "A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27]\n",
    "A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28]\n",
    "Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15]\n",
    "Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29]\n",
    "A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30]\n",
    "Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida.\n",
    "José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33]\n",
    "D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35]\n",
    "Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36]\n",
    "Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37]\n",
    "Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38]\n",
    "D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
    "O imperador teve uma infância solitária e infeliz.[8][42]\n",
    "A perda súbita de seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44]\n",
    "O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
    "\n",
    "Contexto limpo:\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do promptpara o português brasileiro\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=2000,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_openhermes-2-mistral-7b.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_openhermes-2-mistral-7b.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mto(device) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_openhermes-2-mistral-7b.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Geração da resposta\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_openhermes-2-mistral-7b.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m generate_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_openhermes-2-mistral-7b.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_openhermes-2-mistral-7b.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39m\u001b[39m1500\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_openhermes-2-mistral-7b.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     top_k\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_openhermes-2-mistral-7b.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_openhermes-2-mistral-7b.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Decodificação e impressão da resposta\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.254.3.248/var/ontologia/Servicos/ACS_LLMS/server/arquivos/Bench_openhermes-2-mistral-7b.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m answer \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(generate_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1606\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1590\u001b[0m         input_ids,\n\u001b[1;32m   1591\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1603\u001b[0m     )\n\u001b[1;32m   1604\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1605\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1606\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgreedy_search(\n\u001b[1;32m   1607\u001b[0m         input_ids,\n\u001b[1;32m   1608\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1609\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1610\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[1;32m   1611\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m   1612\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[1;32m   1613\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1614\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[1;32m   1615\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[1;32m   1616\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1617\u001b[0m     )\n\u001b[1;32m   1619\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1620\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2515\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2512\u001b[0m         this_peer_finished \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2514\u001b[0m \u001b[39m# stop if we exceed the maximum length\u001b[39;00m\n\u001b[0;32m-> 2515\u001b[0m \u001b[39mif\u001b[39;00m stopping_criteria(input_ids, scores):\n\u001b[1;32m   2516\u001b[0m     this_peer_finished \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2518\u001b[0m \u001b[39mif\u001b[39;00m this_peer_finished \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m synced_gpus:\n",
      "File \u001b[0;32m/var/ontologia/Servicos/ACS_LLMS/server/anaconda3/lib/python3.11/site-packages/transformers/generation/stopping_criteria.py:125\u001b[0m, in \u001b[0;36mStoppingCriteriaList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mStoppingCriteriaList\u001b[39;00m(\u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 125\u001b[0m     \u001b[39m@add_start_docstrings\u001b[39m(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n\u001b[1;32m    126\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_ids: torch\u001b[39m.\u001b[39mLongTensor, scores: torch\u001b[39m.\u001b[39mFloatTensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    127\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39many\u001b[39m(criteria(input_ids, scores) \u001b[39mfor\u001b[39;00m criteria \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m)\n\u001b[1;32m    129\u001b[0m     \u001b[39m@property\u001b[39m\n\u001b[1;32m    130\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mmax_length\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39mint\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "\n",
    "Esta é uma tarefa de retirada de referências bibliográficas do texto. Estas referências estão escritas no formato \"['número']\".\n",
    "\n",
    "Exemplos:\n",
    "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "Contexto: Ontem foi dia de ramos [32].\n",
    "Resposta: Ontem foi dia de ramos.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida. \n",
    "José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
    "O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita de seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
    "\n",
    "Retire as referências do texto dado e escreva a Resposta:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1500,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não retornou resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Esta é uma tarefa de retirada de referências bibliográficas do texto. Estas referências estão escritas no formato \"[número]\".\n",
      "\n",
      "Exemplos:\n",
      "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
      "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
      "Contexto: Ontem foi dia de ramos [32].\n",
      "Resposta: Ontem foi dia de ramos.\n",
      "Contexto: Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming\n",
      "to communicate coherently [1]. These developments have brought about a revolutionary transformation [2] by enabling the creation of Large Language Models (LLMs) that can approximate humanlevel performance on certain evaluation benchmarks [3], [4].\n",
      "Resposta: Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transforbibliográficasmation by enabling the creation of Large Language Models (LLMs) that can approximate humanlevel performance on certain evaluation benchmarks.\n",
      "\n",
      "\n",
      "Q: \n",
      "Retire as referências do texto dado abaixo:\n",
      "\n",
      "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida. José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41] O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita dos seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida. José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitia assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecido facilmente.[41] O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita dos seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
      "\n",
      "\n",
      "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida. José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitia assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecido facilmente.[41] O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita dos seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dad\n"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = r\"\"\"\n",
    "Esta é uma tarefa de retirada de referências bibliográficas do texto. Estas referências estão escritas no formato \"[número]\".\n",
    "\n",
    "Exemplos:\n",
    "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "Contexto: Ontem foi dia de ramos [32].\n",
    "Resposta: Ontem foi dia de ramos.\n",
    "Contexto: Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming\n",
    "to communicate coherently [1]. These developments have brought about a revolutionary transformation [2] by enabling the creation of Large Language Models (LLMs) that can approximate humanlevel performance on certain evaluation benchmarks [3], [4].\n",
    "Resposta: Large Language Models (LLMs) have emerged as cutting-edge artificial intelligence systems designed to process and generate text, aiming to communicate coherently. These developments have brought about a revolutionary transforbibliográficasmation by enabling the creation of Large Language Models (LLMs) that can approximate humanlevel performance on certain evaluation benchmarks.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Retire as referências do texto dado abaixo:\n",
    "\n",
    "Contexto: Ao deixar o país, o imperador D.Pedro I selecionou três pessoas para cuidarem de seu filho e das filhas remanescentes. A primeira foi José Bonifácio de Andrada, seu amigo e líder influente da independência brasileira, nomeado tutor.[26][27] A segunda foi Mariana Carlota de Verna Magalhães Coutinho (depois Condessa de Belmonte), que detinha o cargo de aia desde o nascimento de Pedro II.[28] Quando bebê, D. Pedro II a chamava de \"dadama\", pois não pronunciava corretamente a palavra \"dama\".[15] Considerava-a sua mãe de criação, e continuaria a chamá-la, por afeto, de \"dadama\" mesmo já adulto.[3][29] A terceira pessoa escolhida foi Rafael, um veterano negro da Guerra da Cisplatina.[28][30] Rafael era um empregado do paço em quem D. Pedro I tinha profunda confiança e a quem pediu que olhasse por seu filho — pedido que Rafael levaria a termo pelo resto de sua vida. José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41] O imperador teve uma infância solitária e infeliz.[8][42] A perda súbita dos seus pais o assombraria por toda a vida;[43] ele teve poucos amigos de sua idade[28][36][44] e o contato com suas irmãs era limitado.[31][34][44] O ambiente em que foi criado o tornou tímido e carente,[45][46] enxergando nos livros refúgio e fuga do mundo real.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1500,\n",
    "    top_k=50,\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não conseguiu completar a tarefa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################  EXTRAÇÃO DE CONTEÚDO  ####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto \n",
      "e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A:\n",
      "{\n",
      "\"people\": [\n",
      "  {\n",
      "    \"name\": \"Tim Cook\",\n",
      "    \"gender\": \"male\",\n",
      "    \"image\": \"https://images.apple.com/apple-events/images/apple_events_20220907_tim_cook.jpg\"\n",
      "  }\n",
      "]\n",
      ",\n",
      "\"places\": [\n",
      "  {\n",
      "    \"name\": \"Steve Jobs Theater\",\n",
      "    \"address\": \"10200 North Tantau Ave, Cupertino, CA 95014\"\n",
      "  }\n",
      "]\n",
      ",\n",
      "\"organizations\": [\n",
      "  {\n",
      "    \"name\": \"Apple\",\n",
      "    \"website\": \"https://www.apple.com\"\n",
      "  }\n",
      "]\n",
      "}\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto \n",
      "e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A:\n",
      "{\n",
      "\"people\": [\n",
      "  {\n",
      "    \"name\": \"Tim Cook\",\n",
      "    \"gender\": \"male\",\n",
      "    \"image\": \"https://images.apple.com/apple-events/images/apple_events_20220907_tim_cook.jpg\"\n",
      "  }\n",
      "]\n",
      ",\n",
      "\"places\": [\n",
      "  {\n",
      "    \"name\": \"Steve Jobs Theater\",\n",
      "    \"address\": \"10200 North Tantau Ave, Cupertino, CA 95014\"\n",
      "  }\n",
      "]\n",
      ",\n",
      "\"organizations\": [\n",
      "  {\n",
      "    \"name\": \"Apple\",\n",
      "    \"website\": \"https://www.apple.com\"\n",
      "  }\n",
      "]\n",
      "}\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto \n",
      "e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A:\n",
      "{\n",
      "\"people\": [\n",
      "  {\n",
      "    \"name\": \"Tim Cook\",\n",
      "    \"gender\": \"male\",\n",
      "    \"image\": \"https://images.apple.com/apple-events/images/apple_events_20220907_tim_cook.jpg\"\n",
      "  }\n",
      "]\n",
      ",\n",
      "\"places\": [\n",
      "  {\n",
      "    \"name\": \"Steve Jobs Theater\",\n",
      "    \"address\": \"10200 North Tantau Ave, Cupertino, CA 95014\"\n",
      "  }\n",
      "]\n",
      ",\n",
      "\"organizations\": [\n",
      "  {\n",
      "    \"name\": \"Apple\",\n",
      "    \"website\": \"https://www.apple.com\"\n",
      "  }\n",
      "]\n",
      "}\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto \n",
      "e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A:\n",
      "{\n",
      "\"people\": [\n",
      "  {\n",
      "    \"name\": \"Tim Cook\",\n",
      "    \"gender\": \"male\",\n",
      "    \"image\": \"https://images.apple.com/apple-events/images/apple_events_20220907_tim_cook.jpg\"\n",
      "  }\n",
      "]\n",
      ",\n",
      "\"places\": [\n",
      "  {\n",
      "    \"name\": \"Steve Jobs Theater\",\n",
      "    \"address\": \"10200 North Tantau Ave, Cupertino, CA 95014\"\n",
      "  }\n",
      "]\n",
      ",\n",
      "\"organizations\": [\n",
      "  {\n",
      "    \"name\": \"Apple\",\n",
      "    \"website\": \"https://www.apple.com\"\n",
      "  }\n",
      "]\n",
      "}\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto \n",
      "e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A:\n",
      "{\n",
      "\"people\": [\n",
      "  {\n",
      "    \"name\": \"Tim Cook\",\n",
      "    \"gender\": \"male\",\n",
      "    \"image\": \"https://images\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise o parágrafo, identifique Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto \n",
    "e, em seguida, produza os resultados em formato json.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A: \n",
      "{\n",
      "    \"entities\": {\n",
      "        \"person\": [\n",
      "            \"Tim Cook\"\n",
      "        ],\n",
      "        \"place\": [\n",
      "            \"Steve Jobs Theatre\",\n",
      "            \"Cupertino, California\"\n",
      "        ],\n",
      "        \"organization\": [\n",
      "            \"Apple\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A: \n",
      "{\n",
      "    \"entities\": {\n",
      "        \"person\": [\n",
      "            \"Tim Cook\"\n",
      "        ],\n",
      "        \"place\": [\n",
      "            \"Steve Jobs Theatre\",\n",
      "            \"Cupertino, California\"\n",
      "        ],\n",
      "        \"organization\": [\n",
      "            \"Apple\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A: \n",
      "{\n",
      "    \"entities\": {\n",
      "        \"person\": [\n",
      "            \"Tim Cook\"\n",
      "        ],\n",
      "        \"place\": [\n",
      "            \"Steve Jobs Theatre\",\n",
      "            \"Cupertino, California\"\n",
      "        ],\n",
      "        \"organization\": [\n",
      "            \"Apple\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A: \n",
      "{\n",
      "    \"entities\": {\n",
      "        \"person\": [\n",
      "            \"Tim Cook\"\n",
      "        ],\n",
      "        \"place\": [\n",
      "            \"Steve Jobs Theatre\",\n",
      "            \"Cupertino, California\"\n",
      "        ],\n",
      "        \"organization\": [\n",
      "            \"Apple\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A: \n",
      "{\n",
      "    \"entities\": {\n",
      "        \"person\": [\n",
      "            \"Tim Cook\"\n",
      "        ],\n",
      "        \"place\": [\n",
      "            \"Steve Jobs Theatre\",\n",
      "            \"Cupertino, California\"\n",
      "        ],\n",
      "        \"organization\": [\n",
      "            \"Apple\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A: \n",
      "{\n",
      "    \"entities\": {\n",
      "        \"person\": [\n",
      "            \"Tim Cook\"\n",
      "        ],\n",
      "        \"place\": [\n",
      "            \"Steve Jobs Theatre\",\n",
      "            \"Cupertino, California\"\n",
      "        ],\n",
      "        \"organization\": [\n",
      "            \"Apple\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A: \n",
      "{\n",
      "    \"entities\": {\n",
      "        \"person\": [\n",
      "            \"Tim Cook\"\n",
      "        ],\n",
      "        \"place\": [\n",
      "            \"Steve Jobs Theatre\",\n",
      "            \"Cupertino, California\"\n",
      "        ],\n",
      "        \"organization\": [\n",
      "            \"Apple\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
      "\n",
      "A: \n",
      "{\n",
      "    \"entities\": {\n",
      "        \"person\": [\n",
      "            \"Tim Cook\"\n",
      "        ],\n",
      "       \n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise o parágrafo, identifique Somente as entities dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto e, em seguida, produza os resultados em formato json.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfeito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
      "\n",
      "A:\n",
      "\n",
      "{\n",
      "  \"pessoa\": [\n",
      "    {\n",
      "      \"nome\": \"Tim Cook\",\n",
      "      \"cargo\": \"CEO\"\n",
      "    },\n",
      "    {\n",
      "      \"nome\": \"Steve Jobs\",\n",
      "      \"cargo\": \"CEO\"\n",
      "    }\n",
      "  ],\n",
      "  \"lugar\": [\n",
      "    {\n",
      "      \"nome\": \"Apple\",\n",
      "      \"cidade\": \"Cupertino\"\n",
      "    }\n",
      "  ],\n",
      "  \"organização\": [\n",
      "    {\n",
      "      \"nome\": \"Apple\",\n",
      "      \"cidade\": \"Cupertino\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elon Musk fundou a SpaceX em 2002 com o objetivo de reduzir os custos de transporte espacial e possibilitar a colonização de Marte.\n",
      "\n",
      "\n",
      "Q: \n",
      "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
      "\n",
      "A: \n",
      "\n",
      "```\n",
      "{\n",
      "    \"entities\": {\n",
      "        \"Elon Musk\": {\n",
      "            \"name\": \"Elon Musk\",\n",
      "            \"birth\": \"1971-06-28\"\n",
      "        },\n",
      "        \"SpaceX\": {\n",
      "            \"name\": \"SpaceX\",\n",
      "            \"birth\": \"2002-06-04\"\n",
      "        }\n",
      "    },\n",
      "    \"relationships\": {\n",
      "        \"Elon Musk\": {\n",
      "            \"birth\": \"1971-06-28\"\n",
      "        },\n",
      "        \"SpaceX\": {\n",
      "            \"birth\": \"2002-06-04\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "```\n",
      "{\n",
      "    \"entities\": {\n",
      "        \"Elon Musk\": {\n",
      "            \"name\": \"Elon Musk\",\n",
      "            \"birth\": \"1971-06-28\"\n",
      "        },\n",
      "        \"SpaceX\": {\n",
      "            \"name\": \"SpaceX\",\n",
      "            \"birth\": \"2002-06-04\"\n",
      "        }\n",
      "    },\n",
      "    \"relationships\": {\n",
      "        \"Elon Musk\": {\n",
      "            \"birth\": \"1971-06-28\",\n",
      "            \"birthedBy\": \"SpaceX\"\n",
      "        },\n",
      "        \"SpaceX\": {\n",
      "            \"birth\": \"2002-06-04\",\n",
      "            \"birthedBy\": \"Elon Musk\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "```\n",
      "{\n",
      "    \"entities\": {\n",
      "        \"Elon Musk\": {\n",
      "            \"name\": \"Elon Musk\",\n",
      "            \"birth\": \"1971-06-28\"\n",
      "        },\n",
      "        \"SpaceX\": {\n",
      "            \"name\": \"SpaceX\",\n",
      "            \"birth\": \"2002-06-04\"\n",
      "        }\n",
      "    },\n",
      "    \"relationships\": {\n",
      "        \"Elon Musk\": {\n",
      "            \"birth\": \"1971-06-28\",\n",
      "            \"birthedBy\": \"SpaceX\"\n",
      "        },\n",
      "        \"SpaceX\": {\n",
      "            \"birth\": \"2002-06-04\",\n",
      "            \"birthedBy\": \"Elon Musk\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Extração de Relações #################\n",
    "\n",
    "\n",
    "context = r\"\"\"\n",
    "Elon Musk fundou a SpaceX em 2002 com o objetivo de reduzir os custos de transporte espacial e possibilitar a colonização de Marte.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# Geração da resposta\n",
    "generate_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1000,\n",
    "    #temperature=0.8,\n",
    "    top_k=50,\n",
    "    #top_p=0.95\n",
    ")\n",
    "\n",
    "# Decodificação e impressão da resposta\n",
    "answer = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta:\n",
    "\n",
    "A: \n",
    "\n",
    "\n",
    "{\n",
    "    \"entities\": {\n",
    "        \"Elon Musk\": {\n",
    "            \"name\": \"Elon Musk\",\n",
    "            \"birth\": \"1971-06-28\"\n",
    "        },\n",
    "        \"SpaceX\": {\n",
    "            \"name\": \"SpaceX\",\n",
    "            \"birth\": \"2002-06-04\"\n",
    "        }\n",
    "    },\n",
    "    \"relationships\": {\n",
    "        \"Elon Musk\": {\n",
    "            \"birth\": \"1971-06-28\"\n",
    "        },\n",
    "        \"SpaceX\": {\n",
    "            \"birth\": \"2002-06-04\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "{\n",
    "    \"entities\": {\n",
    "        \"Elon Musk\": {\n",
    "            \"name\": \"Elon Musk\",\n",
    "            \"birth\": \"1971-06-28\"\n",
    "        },\n",
    "        \"SpaceX\": {\n",
    "            \"name\": \"SpaceX\",\n",
    "            \"birth\": \"2002-06-04\"\n",
    "        }\n",
    "    },\n",
    "    \"relationships\": {\n",
    "        \"Elon Musk\": {\n",
    "            \"birth\": \"1971-06-28\",\n",
    "            \"birthedBy\": \"SpaceX\"\n",
    "        },\n",
    "        \"SpaceX\": {\n",
    "            \"birth\": \"2002-06-04\",\n",
    "            \"birthedBy\": \"Elon Musk\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "{\n",
    "    \"entities\": {\n",
    "        \"Elon Musk\": {\n",
    "            \"name\": \"Elon Musk\",\n",
    "            \"birth\": \"1971-06-28\"\n",
    "        },\n",
    "        \"SpaceX\": {\n",
    "            \"name\": \"SpaceX\",\n",
    "            \"birth\": \"2002-06-04\"\n",
    "        }\n",
    "    },\n",
    "    \"relationships\": {\n",
    "        \"Elon Musk\": {\n",
    "            \"birth\": \"1971-06-28\",\n",
    "            \"birthedBy\": \"SpaceX\"\n",
    "        },\n",
    "        \"SpaceX\": {\n",
    "            \"birth\": \"2002-06-04\",\n",
    "            \"birthedBy\": \"Elon Musk\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errou os relacionamentos e trouxe informações fora do texto e sem valor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
