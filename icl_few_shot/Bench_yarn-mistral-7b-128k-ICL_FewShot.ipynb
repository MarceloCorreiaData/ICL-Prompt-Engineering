{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. Yarn Mistral 7B 128k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc375a6236544bdd9019f5c91340bc3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerFast, LlamaTokenizerFast, LlamaForCausalLM, MistralForCausalLM\n",
    "import torch\n",
    "\n",
    "# Configuração do modelo e dispositivo (GPU paralelizada)\n",
    "model_path = '/var/ontologia/Servicos/ACS_LLMS/server/models/yarn-mistral-7b-128k'\n",
    "\n",
    "model = MistralForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(model_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################  COMPREENSÃO, ANÁLISE E GERAÇÃO DE LINGUAGEM NATURAL  #########################################\n",
    "\n",
    "############################################  AVALIAÇÃO DA SEMÂNTICA E SINTAXE EM PT-BR  ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
      "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
      "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
      "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
      "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
      "\n",
      "Pergunta: Com base nos seus conhecimentos e no texto, o que têm em comum LCI e LCA?\n",
      "Resposta: Ambos são títulos emitidos por bancos para financiar o setor imobiliário e agrícola, ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia.\n",
      "\n",
      "\n",
      "Q: \n",
      "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique qual é o investimento mais adequado para alguém que busca proteção contra a inflação.\n",
      "\n",
      "A: \n",
      "O investimento mais adequado para alguém que busca proteção contra a inflação é o Tesouro IPCA+. Esse tipo de título é ideal para quem deseja proteger seu dinheiro contra a inflação, pois o rendimento é acordado com base no índice de preços ao consumidor (IPCA). Além disso, o Tesouro IPCA+ é considerado um dos investimentos mais seguros do país, devido à garantia do governo federal.\n",
      "\n",
      "\n",
      "Q: \n",
      "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique qual é o invest\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering: Compreensão de texto e xtração de conteúdo com Contexto de Domínio #################\n",
    "\n",
    "# Contexto e questão\n",
    "context = \"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\n",
    "Pergunta: Com base nos seus conhecimentos e no texto, o que têm em comum LCI e LCA?\n",
    "Resposta: Ambos são títulos emitidos por bancos para financiar o setor imobiliário e agrícola, ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique qual é o investimento mais adequado para alguém que busca proteção contra a inflação.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenizar o texto\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=160,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
      "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
      "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
      "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
      "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
      "\n",
      "Pergunta: Com base nos seus conhecimentos e no texto, o que têm em comum LCI e LCA?\n",
      "Resposta: Ambos são títulos emitidos por bancos para financiar o setor imobiliário e agrícola, ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia.\n",
      "\n",
      "\n",
      "Q: \n",
      "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique o que é Tesouro Selic (em português).\n",
      "\n",
      "A: \n",
      "O Tesouro Selic é um título público emitido pelo governo federal, ideal para reserva de emergência. Ele é nomeado assim porque sua taxa de juros é acordada com base no Selic, o principal indicador de taxa de juros do país. O Tesouro Selic é um investimento seguro, pois é garantido pelo governo federal. Ele é um dos investimentos mais seguros do país, pois a taxa de juros é acordada com base no Selic, o principal indicador de taxa de juros do país. Além disso, o Tesouro Selic é um investimento líquido, pois pode ser resgatado a qualquer momento.\n",
      "\n",
      "\n",
      "Q: \n",
      "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique o que é Tesouro Prefixado (em português).\n",
      "\n",
      "A: \n",
      "O Tesouro Prefixado é um título público emitido pelo governo federal, ideal para quem acredita que a taxa de juros e a\n"
     ]
    }
   ],
   "source": [
    "################# Question Answering com Contexto de Domínio #################\n",
    "\n",
    "# Contexto e questão\n",
    "context = \"\"\"\n",
    "Os investimentos em renda fixa são considerados uma das opções mais seguras do mercado financeiro e são especialmente atraentes para investidores com perfil conservador. Esses investimentos são compostos principalmente por títulos de dívida, seja pública ou privada. No Brasil, muitos bancos, incluindo o Banco do Brasil, oferecem uma variedade de opções de investimento em renda fixa. Esses investimentos têm como característica principal o retorno previsível, e o rendimento, na maioria das vezes, é acordado no momento da aplicação.\n",
    "- **CDB (Certificado de Depósito Bancário)**: O CDB pode ser encontrado em 3 modalidades: prefixada, pós-fixada e progressiva. Enquanto o CDB prefixado tem um rendimento acordado no ato da aplicação, o pós-fixado e o progressivo são atrelados ao CDI, com o último tendo sua taxa definida com base no tempo de permanência da aplicação. Apesar do prazo de vencimento, o CDB pós-fixado e o progressivo podem ser resgatados a qualquer momento. É adequado para investidores que buscam mais rentabilidade do que a poupança e estão dispostos a abrir mão da liquidez diária. \n",
    "- **LCI e LCA**: Ambos são títulos emitidos por bancos para financiar respectivamente o setor imobiliário e agrícola. São ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia, como imobiliário ou agrícola. \n",
    "- **Títulos do Tesouro Direto**: Existem diversas modalidades, como o Tesouro Selic, ideal para reserva de emergência; Tesouro Prefixado, para quem acredita que a taxa de juros e a inflação vão cair, e Tesouro IPCA+, indicado para quem deseja proteger seu dinheiro contra a inflação. Recomendados para quem busca diversificar sua carteira de investimentos com opções seguras. São títulos públicos emitidos pelo governo federal e são considerados os investimentos mais seguros do país devido à garantia do governo federal. \n",
    "A maioria desses investimentos tem a proteção do FGC (Fundo Garantidor de Créditos), o que os torna ainda mais seguros para os investidores. Além disso, o Banco do Brasil, uma das principais instituições financeiras do país, oferece todas essas opções de investimento com diferentes vantagens e condições, atendendo a diversos perfis de investidores, desde os mais conservadores até os mais arrojados.\n",
    "\n",
    "Pergunta: Com base nos seus conhecimentos e no texto, o que têm em comum LCI e LCA?\n",
    "Resposta: Ambos são títulos emitidos por bancos para financiar o setor imobiliário e agrícola, ideais para investidores que buscam isenção de Imposto de Renda e desejam investir em setores específicos da economia.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Com base nos seus conhecimentos e no texto apresentado sobre investimentos em reda fixa, explique o que é Tesouro Selic (em português).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinaçãanswer = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)o do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Olá\n",
      "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
      "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
      "Como posso atualizar meu cartão de crédito?\n",
      "Desde já, obrigado,\n",
      "João\n",
      "\n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: \n",
      "Olá João,\n",
      "\n",
      "Nós estamos felizes em ajudá-lo.\n",
      "\n",
      "Para atualizar seu cartão de crédito, você pode seguir os seguintes passos:\n",
      "\n",
      "1. Entre em sua conta no site da empresa.\n",
      "2. Clique em \"Configurações\" ou \"Perfil\" no menu superior.\n",
      "3. Clique em \"Pagamento\" ou \"Cartão de Crédito\".\n",
      "4. Clique em \"Editar\" ou \"Atualizar\" ao lado do cartão de crédito atual.\n",
      "5. Insira os detalhes do novo cartão de crédito, incluindo o número do cartão, o nome do titular, a data de vencimento e o código de segurança.\n",
      "6. Clique em \"Salvar\" ou \"Atualizar\" para salvar as alterações.\n",
      "\n",
      "Se você tiver alguma dúvida ou precisar de ajuda adicional, não hesite em entrar em contato com o suporte da empresa.\n",
      "\n",
      "Esperamos que isso ajude\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto:\n",
      "\"Estou tendo problemas para fazer login na minha conta, e já tentei redefinir minha senha várias vezes sem sucesso. O que mais eu posso fazer para resolver isso?\"\n",
      "Intenção: Solicitar assistência técnica.\n",
      "\n",
      "Texto:\n",
      "\"Acabei de receber o produto que comprei de vocês, e estou muito satisfeito com a qualidade. Quero agradecer e dizer que continuarei sendo um cliente fiel.\"\n",
      "Intenção: Expressar satisfação e gratidão.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Olá\n",
      "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
      "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
      "Como posso atualizar meu cartão de crédito?\n",
      "Desde já, obrigado,\n",
      "João\n",
      "\n",
      "Classifique a intenção presente no texto.\n",
      "\n",
      "A: Solicitar assistência técnica\n",
      "\n",
      "B: Expressar satisfação e gratidão\n",
      "\n",
      "C: Solicitar informações\n",
      "\n",
      "D: Solicitar uma alteração\n",
      "\n",
      "E: Solicitar uma atualização\n",
      "\n",
      "F: Solicitar uma revisão\n",
      "\n",
      "G: Solicitar uma correção\n",
      "\n",
      "H: Solicitar uma atualização\n",
      "\n",
      "I: Solicitar uma atualização\n",
      "\n",
      "J: Solicitar uma atualização\n",
      "\n",
      "K: Solicitar uma atualização\n",
      "\n",
      "L: Solicitar uma atualização\n",
      "\n",
      "M: Solicitar uma atualização\n",
      "\n",
      "N: Solicitar uma atualização\n",
      "\n",
      "O: Solicitar uma atualização\n",
      "\n",
      "P: Solicitar uma atualização\n",
      "\n",
      "Q: Solicitar uma atualização\n",
      "\n",
      "R: Solicitar uma atualização\n",
      "\n",
      "S: Solicitar uma atualização\n",
      "\n",
      "T: Solicitar uma atualização\n",
      "\n",
      "U: Solicitar uma atualização\n",
      "\n",
      "V: Solicitar uma atualização\n",
      "\n",
      "W\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Estou tendo problemas para fazer login na minha conta, e já tentei redefinir minha senha várias vezes sem sucesso. O que mais eu posso fazer para resolver isso?\"\n",
    "Intenção: Solicitar assistência técnica.\n",
    "\n",
    "Texto:\n",
    "\"Acabei de receber o produto que comprei de vocês, e estou muito satisfeito com a qualidade. Quero agradecer e dizer que continuarei sendo um cliente fiel.\"\n",
    "Intenção: Expressar satisfação e gratidão.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João\n",
    "\n",
    "Classifique a intenção presente no texto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<Exemplos>>\n",
      "Texto: Quero cancelar minha compra recente.\n",
      "Intenção: Ajuda com entendimento de fatura\n",
      "\n",
      "Texto: Como faço para acompanhar minha entrega?\n",
      "Intenção: Problemas com aplicativo móvel\n",
      "\n",
      "Texto: Preciso de ajuda para entender minha fatura.\n",
      "Intenção: Cancelamento de compra\n",
      "\n",
      "Texto: Quero informações sobre a garantia do produto.\n",
      "Intenção: Alteração de endereço de entrega\n",
      "\n",
      "Texto: Como posso atualizar minhas informações de pagamento?\n",
      "Intenção: Horários de funcionamento do suporte\n",
      "\n",
      "Texto: Estou tentando agendar uma consulta, mas não consigo.\n",
      "Intenção: Informações sobre garantia de produto\n",
      "\n",
      "Texto: Posso trocar um produto que comprei online?\n",
      "Intenção: Acompanhamento de entrega\n",
      "\n",
      "Texto: Como posso atualizar minhas informações de pagamento?\n",
      "Intenção: Troca de produto comprado online\n",
      "\n",
      "Texto: Quais são os horários de funcionamento do suporte?\n",
      "Intenção: Alteração de endereço de entrega\n",
      "\n",
      "Texto: Estou tentando agendar uma consulta, mas não consigo.\n",
      "Intenção: Informações sobre garantia de produto\n",
      "\n",
      "Texto: Estou tendo problemas com o aplicativo móvel.\n",
      "Intenção: Troca de produto comprado online\n",
      "\n",
      "Texto: Preciso de ajuda para entender minha fatura.\n",
      "Intenção: Problemas com aplicativo móvel\n",
      "\n",
      "Texto: Posso trocar um produto que comprei online?\n",
      "Intenção: Informações sobre garantia de produto\n",
      "\n",
      "Texto: Quero cancelar minha compra recente.\n",
      "Intenção: Ajuda com entendimento de fatura\n",
      "\n",
      "Texto: Quais são os horários de funcionamento do suporte?\n",
      "Intenção: Ajuda com entendimento de fatura\n",
      "\n",
      "Texto: Quero cancelar minha compra recente.\n",
      "Intenção: Troca de produto comprado online\n",
      "\n",
      "Texto: Estou tendo problemas com o aplicativo móvel.\n",
      "Intenção: Alteração de endereço de entrega\n",
      "\n",
      "Texto: Posso trocar um produto que comprei online?\n",
      "Intenção: Cancelamento de compra\n",
      "\n",
      "Texto: Como faço para alterar meu endereço de entrega?\n",
      "Intenção: Alteração de endereço de entrega\n",
      "\n",
      "Texto: Quero cancelar minha compra recente.\n",
      "Intenção: Atualização de informações de pagamento\n",
      "\n",
      "Texto: Quais são os horários de funcionamento do suporte?\n",
      "Intenção: Acompanhamento de entrega\n",
      "\n",
      "Texto: Quais são os horários de funcionamento do suporte?\n",
      "Intenção: Ajuda com entendimento de fatura\n",
      "\n",
      "Texto: Quais são os horários de funcionamento do suporte?\n",
      "Intenção: Atualização de informações de pagamento\n",
      "\n",
      "Texto: Posso trocar um produto que comprei online?\n",
      "Intenção: Atualização de informações de pagamento\n",
      "\n",
      "Texto: Como posso atualizar minhas informações de pagamento?\n",
      "Intenção: Informações sobre garantia de produto\n",
      "\n",
      "Texto: Quero cancelar minha compra recente.\n",
      "Intenção: Ajuda com entendimento de fatura\n",
      "\n",
      "Texto: Quero cancelar minha compra recente.\n",
      "Intenção: Atualização de informações de pagamento\n",
      "\n",
      "Texto: Estou tendo problemas com o aplicativo móvel.\n",
      "Intenção: Acompanhamento de entrega\n",
      "\n",
      "Texto: Como posso atualizar minhas informações de pagamento?\n",
      "Intenção: Troca de produto comprado online\n",
      "\n",
      "Texto: Quero informações sobre a garantia do produto.\n",
      "Intenção: Problemas com aplicativo móvel\n",
      "\n",
      "\n",
      "Q: \n",
      "<<Situação Real>>\n",
      "Olá\n",
      "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
      "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
      "Como posso atualizar meu cartão de crédito?\n",
      "Desde já, obrigado,\n",
      "João\n",
      "\n",
      "\n",
      "De acordo com os <<Exemplos>>, classifique a intenção de João no texto apresentado na <<Situração Real>>.\n",
      "\n",
      "A: Ajuda com entendimento de fatura\n",
      "B: Alteração de endereço de entrega\n",
      "C: Atualização de informações de pagamento\n",
      "D: Acompanhamento de entrega\n",
      "E: Troca de produto comprado online\n",
      "F: Problemas com aplicativo móvel\n",
      "G: Informações sobre garantia de produto\n",
      "H: Cancelamento de compra\n",
      "I: Horários de funcionamento do suporte\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Classificação de Intenções #################\n",
    "\n",
    "context = \"\"\"\n",
    "<<Exemplos>>\n",
    "Texto: Quero cancelar minha compra recente.\n",
    "Intenção: Ajuda com entendimento de fatura\n",
    "\n",
    "Texto: Como faço para acompanhar minha entrega?\n",
    "Intenção: Problemas com aplicativo móvel\n",
    "\n",
    "Texto: Preciso de ajuda para entender minha fatura.\n",
    "Intenção: Cancelamento de compra\n",
    "\n",
    "Texto: Quero informações sobre a garantia do produto.\n",
    "Intenção: Alteração de endereço de entrega\n",
    "\n",
    "Texto: Como posso atualizar minhas informações de pagamento?\n",
    "Intenção: Horários de funcionamento do suporte\n",
    "\n",
    "Texto: Estou tentando agendar uma consulta, mas não consigo.\n",
    "Intenção: Informações sobre garantia de produto\n",
    "\n",
    "Texto: Posso trocar um produto que comprei online?\n",
    "Intenção: Acompanhamento de entrega\n",
    "\n",
    "Texto: Como posso atualizar minhas informações de pagamento?\n",
    "Intenção: Troca de produto comprado online\n",
    "\n",
    "Texto: Quais são os horários de funcionamento do suporte?\n",
    "Intenção: Alteração de endereço de entrega\n",
    "\n",
    "Texto: Estou tentando agendar uma consulta, mas não consigo.\n",
    "Intenção: Informações sobre garantia de produto\n",
    "\n",
    "Texto: Estou tendo problemas com o aplicativo móvel.\n",
    "Intenção: Troca de produto comprado online\n",
    "\n",
    "Texto: Preciso de ajuda para entender minha fatura.\n",
    "Intenção: Problemas com aplicativo móvel\n",
    "\n",
    "Texto: Posso trocar um produto que comprei online?\n",
    "Intenção: Informações sobre garantia de produto\n",
    "\n",
    "Texto: Quero cancelar minha compra recente.\n",
    "Intenção: Ajuda com entendimento de fatura\n",
    "\n",
    "Texto: Quais são os horários de funcionamento do suporte?\n",
    "Intenção: Ajuda com entendimento de fatura\n",
    "\n",
    "Texto: Quero cancelar minha compra recente.\n",
    "Intenção: Troca de produto comprado online\n",
    "\n",
    "Texto: Estou tendo problemas com o aplicativo móvel.\n",
    "Intenção: Alteração de endereço de entrega\n",
    "\n",
    "Texto: Posso trocar um produto que comprei online?\n",
    "Intenção: Cancelamento de compra\n",
    "\n",
    "Texto: Como faço para alterar meu endereço de entrega?\n",
    "Intenção: Alteração de endereço de entrega\n",
    "\n",
    "Texto: Quero cancelar minha compra recente.\n",
    "Intenção: Atualização de informações de pagamento\n",
    "\n",
    "Texto: Quais são os horários de funcionamento do suporte?\n",
    "Intenção: Acompanhamento de entrega\n",
    "\n",
    "Texto: Quais são os horários de funcionamento do suporte?\n",
    "Intenção: Ajuda com entendimento de fatura\n",
    "\n",
    "Texto: Quais são os horários de funcionamento do suporte?\n",
    "Intenção: Atualização de informações de pagamento\n",
    "\n",
    "Texto: Posso trocar um produto que comprei online?\n",
    "Intenção: Atualização de informações de pagamento\n",
    "\n",
    "Texto: Como posso atualizar minhas informações de pagamento?\n",
    "Intenção: Informações sobre garantia de produto\n",
    "\n",
    "Texto: Quero cancelar minha compra recente.\n",
    "Intenção: Ajuda com entendimento de fatura\n",
    "\n",
    "Texto: Quero cancelar minha compra recente.\n",
    "Intenção: Atualização de informações de pagamento\n",
    "\n",
    "Texto: Estou tendo problemas com o aplicativo móvel.\n",
    "Intenção: Acompanhamento de entrega\n",
    "\n",
    "Texto: Como posso atualizar minhas informações de pagamento?\n",
    "Intenção: Troca de produto comprado online\n",
    "\n",
    "Texto: Quero informações sobre a garantia do produto.\n",
    "Intenção: Problemas com aplicativo móvel\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "<<Situação Real>>\n",
    "Olá\n",
    "Passei algum tempo analisando sua documentação, mas não consegui descobrir como adicionar um novo cartão de crédito.\n",
    "É um problema porque meu cartão atual irá expirar em breve e temo que isso cause uma interrupção no serviço.\n",
    "Como posso atualizar meu cartão de crédito?\n",
    "Desde já, obrigado,\n",
    "João\n",
    "\n",
    "\n",
    "De acordo com os <<Exemplos>>, classifique a intenção de João no texto apresentado na <<Situração Real>>.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
      "\n",
      "Pergunta: Por que a Grande Barreira de Corais pode ser considerada importante para a biodiversidade marinha?\n",
      "Resposta: A Grande Barreira de Corais é importante para a biodiversidade marinha devido à sua grande variedade de vida marinha, incluindo um número significativo de espécies de peixes e corais, o que indica um ecossistema rico e diversificado.\n",
      "\n",
      "Pergunta: Como a localização da Grande Barreira de Corais afeta sua biodiversidade?\n",
      "Resposta: Sua localização no Mar de Coral, que possui condições ideais como águas quentes e claras, favorece a biodiversidade, permitindo que um grande número de espécies de corais e peixes prospere.\n",
      "\n",
      "\n",
      "Q: \n",
      "Dada a descrição do contexto acima:\n",
      "Pergunta: Explique como a diversidade de espécies na Grande Barreira de Corais contribui para a sua importância como um ecossistema marinho.\n",
      "\n",
      "A: A Grande Barreira de Corais é um ecossistema marinho rico e diversificado, com uma grande variedade de espécies de peixes, corais e outras formas de vida marinha. Essa diversidade de espécies é importante porque cada espécie desempenha um papel único no ecossistema, contribuindo para a estabilidade e a saúde do ambiente. Por exemplo, os corais fornecem abrigo e alimentação para uma grande variedade de peixes, enquanto os peixes desempenham um papel importante na limpeza dos recifes de corais e na dispersão de sementes de cor\n"
     ]
    }
   ],
   "source": [
    "################# Compreensão e Geração de Texto #################\n",
    "\n",
    "context = \"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\n",
    "Pergunta: Por que a Grande Barreira de Corais pode ser considerada importante para a biodiversidade marinha?\n",
    "Resposta: A Grande Barreira de Corais é importante para a biodiversidade marinha devido à sua grande variedade de vida marinha, incluindo um número significativo de espécies de peixes e corais, o que indica um ecossistema rico e diversificado.\n",
    "\n",
    "Pergunta: Como a localização da Grande Barreira de Corais afeta sua biodiversidade?\n",
    "Resposta: Sua localização no Mar de Coral, que possui condições ideais como águas quentes e claras, favorece a biodiversidade, permitindo que um grande número de espécies de corais e peixes prospere.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Dada a descrição do contexto acima:\n",
    "Pergunta: Explique como a diversidade de espécies na Grande Barreira de Corais contribui para a sua importância como um ecossistema marinho.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=160,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
      "\n",
      "Questão de Compreensão de Texto para vocẽ, Modelo de Linguagem:\n",
      "Pergunta: Explique como a diversidade de espécies na Grande Barreira de Corais contribui para a sua importância como um ecossistema marinho.\n",
      "Sua resposta:\n",
      "A Grande Barreira de Corais é importante como um ecossistema marinho devido à sua grande diversidade de espécies. Com mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies, o recife abriga um ecossistema rico e diversificado. Essa diversidade de espécies significa que a Grande Barreira de Corais é um importante habitat para muitas espécies de vida marinha, e que a perda de qualquer espécie pode ter consequências significativas para o ecossistema como um todo.\n",
      "\n",
      "\n",
      "Q: \n",
      "Questão de Autoavaliação (Self-reflection) para você, Modelo de Linguagem:\n",
      "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
      "\n",
      "A:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\n",
    "Questão de Compreensão de Texto para vocẽ, Modelo de Linguagem:\n",
    "Pergunta: Explique como a diversidade de espécies na Grande Barreira de Corais contribui para a sua importância como um ecossistema marinho.\n",
    "Sua resposta:\n",
    "A Grande Barreira de Corais é importante como um ecossistema marinho devido à sua grande diversidade de espécies. Com mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies, o recife abriga um ecossistema rico e diversificado. Essa diversidade de espécies significa que a Grande Barreira de Corais é um importante habitat para muitas espécies de vida marinha, e que a perda de qualquer espécie pode ter consequências significativas para o ecossistema como um todo.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Questão de Autoavaliação (Self-reflection) para você, Modelo de Linguagem:\n",
    "Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
      "A: 0.8\n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
      "A: 0.8\n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\n",
      "A: 0.8\n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avali\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão. \\\n",
    "Além da autoavaliação de nota, responda a minha questão: quais dificuldades ou limitações você reconhece ter em relação à compreensão de texto?\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################  INFERÊNCIA LÓGICA  ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Afirmações: P ⊃ Q e Q ⊃ P\n",
      "Pergunta: Determine se as afirmações são logicamente equivalentes, contraditórias, consistentes ou inconsistentes.\n",
      "Resposta: (A) Logicamente equivalente, porque se P então Q e se Q então P implicam um ao outro.\n",
      "\n",
      "Afirmações: P · Q e ∼P v ∼Q (onde 'v' representa 'ou')\n",
      "Pergunta: Determine se as afirmações são logicamente equivalentes, contraditórias, consistentes ou inconsistentes.\n",
      "Resposta: (B) Contraditório, porque P e Q não podem ser verdadeiros ao mesmo tempo que não P ou não Q.\n",
      "\n",
      "\n",
      "Q: \n",
      "Esta é uma tarefa de lógica formal.\n",
      "Determine se as duas afirmações são logicamente equivalentes ou contraditórias. Se não, determine se são consistentes ou inconsistentes.\n",
      "\n",
      "Afirmações:\n",
      "\n",
      "E ⊃ (F · E) e ∼ E · F\n",
      "\n",
      "Aponte a alternativa correta e explique a solução:\n",
      "(A) Logicamente equivalentes\n",
      "(B) Contraditórias\n",
      "(C) Nem logicamente equivalente nem contraditórias, mas consistentes\n",
      "(D) Inconsistentes\n",
      "\n",
      "\n",
      "A: (B) Contraditório, porque E ⊃ (F · E) e ∼ E · F implicam um ao outro.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Lógica Formal (do MMLU Benchmark) #################\n",
    "\n",
    "context = \"\"\" \n",
    "Afirmações: P ⊃ Q e Q ⊃ P\n",
    "Pergunta: Determine se as afirmações são logicamente equivalentes, contraditórias, consistentes ou inconsistentes.\n",
    "Resposta: (A) Logicamente equivalente, porque se P então Q e se Q então P implicam um ao outro.\n",
    "\n",
    "Afirmações: P · Q e ∼P v ∼Q (onde 'v' representa 'ou')\n",
    "Pergunta: Determine se as afirmações são logicamente equivalentes, contraditórias, consistentes ou inconsistentes.\n",
    "Resposta: (B) Contraditório, porque P e Q não podem ser verdadeiros ao mesmo tempo que não P ou não Q.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Esta é uma tarefa de lógica formal.\n",
    "Determine se as duas afirmações são logicamente equivalentes ou contraditórias. Se não, determine se são consistentes ou inconsistentes.\n",
    "\n",
    "Afirmações:\n",
    "\n",
    "E ⊃ (F · E) e ∼ E · F\n",
    "\n",
    "Aponte a alternativa correta e explique a solução:\n",
    "(A) Logicamente equivalentes\n",
    "(B) Contraditórias\n",
    "(C) Nem logicamente equivalente nem contraditórias, mas consistentes\n",
    "(D) Inconsistentes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
      "---- Medicina\n",
      "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
      "---- Finanças\n",
      "\n",
      "\n",
      "Q: \n",
      "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
      "---- [Categoria a ser encontrada]\n",
      "Substitua o termo [Categoria a ser encontrada]\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "Q: \n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Inferência Lógica #################\n",
    "\n",
    "context = \"\"\"\n",
    "Dados divulgados pelo Departamento Nacional de Seguro de Saúde mostraram que de janeiro a março de 2023, a receita total do fundo de seguro médico básico (incluindo seguro de maternidade) foi de 910,48 bilhões de yuans, um aumento de 9,5% em relação ao ano anterior.\n",
    "---- Medicina\n",
    "A Berkshire Hathaway reduziu sua participação nas ações da BYD de 10,05% para 9,87%. A HKEx Disclosure Ease exige divulgação quando um acionista majoritário aumenta ou diminui sua participação acionária somente se ultrapassar uma determinada porcentagem.\n",
    "---- Finanças\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Os produtos de edge computing têm sido aplicados em setores como educação on-line, vídeo, jogos e carros conectados.\n",
    "---- [Categoria a ser encontrada]\n",
    "Substitua o termo [Categoria a ser encontrada]\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
      "A: 0.8\n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
      "A: 0.8\n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
      "A: 0.8\n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
      "A: 0.8\n",
      "\n",
      "Q: Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\n",
      "A: 0.8\n",
      "\n",
      "Q: Em seguida, faça uma auto\n"
     ]
    }
   ],
   "source": [
    "#################  ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\" \"\"\"\n",
    "question = \"Em seguida, faça uma autoavaliação e atribua-se uma nota de 0.00 a 1.00, avaliando a qualidade da sua resposta à questão.\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A criação de questões de múltipla escolha para exames de história requer não apenas um entendimento profundo do período histórico, mas também a habilidade de formular perguntas que avaliem adequadamente o conhecimento dos alunos. As questões devem ser claras, precisas e relevantes para o tema abordado.\n",
      "\n",
      "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renascimento, com mais de uma alternativa correta, obrigatoriamente.\n",
      "Demonstração: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "A resposta correta é: A e C.\n",
      "Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
      "\n",
      "\n",
      "Q: \n",
      "Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\n",
      "Instruções: Crie uma Questão de múltipla escolha para uma prova de história.\n",
      "Relevância: A questão deveria estar relacionada ao Renascimento.\n",
      "Restrição: Deve haver mais de duas respostas corretas entre as quatro opções. Oriente o aluno a assinalá-las.\n",
      "\n",
      "A: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "\n",
      "B: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "\n",
      "C: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "\n",
      "D: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "\n",
      "E: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "\n",
      "F: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "\n",
      "G: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "\n",
      "H: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "\n",
      "I: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "\n",
      "J: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
      "\n",
      "K: \n",
      "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
      "  (A) Londres (B) Paris\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão de Intenções (com Demonstração) #################\n",
    "\n",
    "context = \"\"\"\n",
    "A criação de questões de múltipla escolha para exames de história requer não apenas um entendimento profundo do período histórico, mas também a habilidade de formular perguntas que avaliem adequadamente o conhecimento dos alunos. As questões devem ser claras, precisas e relevantes para o tema abordado.\n",
    "\n",
    "Pergunta: Crie uma pergunta de múltipla escolha sobre o Renascimento, com mais de uma alternativa correta, obrigatoriamente.\n",
    "Demonstração: \n",
    "Durante o Renascimento, qual das seguintes cidades era um centro cultural?\n",
    "  (A) Londres (B) Paris (C) Florença (D) Amsterdã\n",
    "A resposta correta é: A e C.\n",
    "Explicação: Durante o Renascimento, Florença foi um dos centros culturais mais importantes da Europa, onde muitos artistas, estudiosos e pensadores importantes trabalharam e viveram, incluindo Leonardo da Vinci, Michelangelo e Dante. Londres também cresceu rapidamente durante a Renascença para se tornar um centro cultural e comercial da Europa, atraindo muitas celebridades culturais e comerciantes. Portanto, as opções A e C são as respostas corretas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Suponha que você seja um professor de história com mais de 30 anos de experiência docente.\n",
    "Instruções: Crie uma Questão de múltipla escolha para uma prova de história.\n",
    "Relevância: A questão deveria estar relacionada ao Renascimento.\n",
    "Restrição: Deve haver mais de duas respostas corretas entre as quatro opções. Oriente o aluno a assinalá-las.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=512,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frases para Agrupar:\n",
      "1. \"A produção industrial está em alta este mês.\"\n",
      "2. \"Agricultores relatam uma colheita abundante nesta temporada.\"\n",
      "3. \"O crescimento econômico acelerou no último trimestre.\"\n",
      "\n",
      "Agrupamento:\n",
      "    Frases 1 e 3: Ambas tratam de crescimento econômico.\n",
      "    Frase 2: Trata de agricultura, não se encaixa diretamente com as outras duas sobre economia.\n",
      "\n",
      "Frases para Agrupar:\n",
      "1. \"Pesquisadores descobrem um novo tipo de antibiótico no solo da floresta tropical.\"\n",
      "2. \"A conferência de tecnologia revela inovações em inteligência artificial.\"\n",
      "3. \"Novo método de reciclagem aumenta a eficiência na conversão de resíduos plásticos.\"\n",
      "\n",
      "Agrupamento:\n",
      "    Frase 1: Relacionada a descobertas científicas.\n",
      "    Frases 2 e 3: Relacionadas a inovações tecnológicas.\n",
      "\n",
      "\n",
      "Q: \n",
      "Frases para Agrupar:\n",
      "\n",
      "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
      "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
      "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
      "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
      "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
      "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
      "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
      "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
      "\n",
      "Pergunta:\n",
      "Esta é uma tarefa de agrupamento de frases similares. Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
      "\n",
      "A:\n",
      "\n",
      "Frases para Agrupar:\n",
      "\n",
      "1. \"A economia global está em recessão devido à pandemia.\"\n",
      "2. \"O preço do petróleo caiu drasticamente devido à crise econômica.\"\n",
      "3. \"O governo anunciou um pacote de estímulo para ajudar os trabalhadores afetados pela crise.\"\n",
      "4. \"A indústria de turismo sofreu um grande impacto devido à pandemia.\"\n",
      "5. \"O mercado de ações sofreu uma queda significativa devido à crise econômica.\"\n",
      "6. \"O governo anunciou uma moratória em pagamentos de empréstimos para ajudar os trabalhadores afetados pela crise.\"\n",
      "7. \"A indústria de construção sofreu um grande impacto devido à crise econômica.\"\n",
      "8. \"O governo anunciou um programa de desemprego para ajudar os trabalhadores afetados pela crise.\"\n",
      "\n",
      "Agrupamento:\n"
     ]
    }
   ],
   "source": [
    "################# Categorização e Clustering / Agrupamento #################\n",
    "\n",
    "context = \"\"\"\n",
    "Frases para Agrupar:\n",
    "1. \"A produção industrial está em alta este mês.\"\n",
    "2. \"Agricultores relatam uma colheita abundante nesta temporada.\"\n",
    "3. \"O crescimento econômico acelerou no último trimestre.\"\n",
    "\n",
    "Agrupamento:\n",
    "    Frases 1 e 3: Ambas tratam de crescimento econômico.\n",
    "    Frase 2: Trata de agricultura, não se encaixa diretamente com as outras duas sobre economia.\n",
    "\n",
    "Frases para Agrupar:\n",
    "1. \"Pesquisadores descobrem um novo tipo de antibiótico no solo da floresta tropical.\"\n",
    "2. \"A conferência de tecnologia revela inovações em inteligência artificial.\"\n",
    "3. \"Novo método de reciclagem aumenta a eficiência na conversão de resíduos plásticos.\"\n",
    "\n",
    "Agrupamento:\n",
    "    Frase 1: Relacionada a descobertas científicas.\n",
    "    Frases 2 e 3: Relacionadas a inovações tecnológicas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Frases para Agrupar:\n",
    "\n",
    "\"As vendas de carros elétricos da Tesla continuam a subir apesar da pandemia\"\n",
    "\"Novo estudo sugere que o consumo de café pode diminuir o risco de doenças cardíacas\"\n",
    "\"O modelo mais recente do iPhone apresenta uma tela maior e câmera aprimorada\"\n",
    "\"Casos de COVID-19 aumentam na Índia, sobrecarregando o sistema de saúde\"\n",
    "\"Amazon anuncia planos para construir um novo centro de distribuição no Texas\"\n",
    "\"Cientistas descobrem novas espécies de aves na floresta amazônica\"\n",
    "\"Conferência global sobre mudanças climáticas será realizada em Paris no próximo mês\"\n",
    "\"Starbucks apresenta opções de leite vegetal em todas as lojas dos EUA\"\n",
    "\n",
    "Pergunta:\n",
    "Esta é uma tarefa de agrupamento de frases similares. Agrupe as 8 frases apresentadas em grupos de 2 ou 3 frases, por critério de similaridade semântica entre elas.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################## SUMARIZAÇÃO DE TEXTO ##################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto: \"A água é essencial para a vida na Terra. Ela não apenas sustenta os organismos vivos, mas também participa de vários processos vitais, como o ciclo hidrológico, que afeta o clima global e as condições meteorológicas.\"\n",
      "Pergunta: Resuma o texto.\n",
      "Resposta: A água é crucial para a vida e processos globais como o clima.\n",
      "\n",
      "Texto: \"As florestas tropicais são ricas em biodiversidade. Elas não só abrigam uma grande variedade de espécies animais e vegetais, mas também ajudam a regular o clima terrestre e são vitais para a manutenção de muitos ecossistemas.\"\n",
      "Pergunta: Resuma o texto.\n",
      "Resposta: Florestas tropicais têm alta biodiversidade e são importantes para o clima e ecossistemas.\n",
      "\n",
      "\n",
      "Q: \n",
      "Texto: \"Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
      "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
      "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\"\n",
      "\n",
      "Pergunta: Resuma o texto.\n",
      "\n",
      "A: A história conta a vida de um homem que perdeu a sua pipa. Ele se lembra de quando era jovem e como ele perdeu a pipa. Ele se lembra de como ele se sentiu quando perdeu a pipa e como ele tentou encontrá-la. Ele se lembra de como ele se sentiu quando perdeu a pipa e como ele tentou encontrá-la. Ele se lembra de como ele se sentiu quando perdeu a pipa e como ele tentou encontrá-la. Ele se lembra de como ele se sentiu quando perdeu a pipa e como ele tentou encontrá-la. Ele se lembra de como ele se sentiu quando perdeu a pipa e como ele tentou encontrá-la. Ele se lembra de como ele se sentiu quando perdeu a pipa e como ele tentou encontrá-la. Ele se lembra de como ele se sentiu quando perdeu a pipa e como ele tentou encontrá-la. Ele se lembra de como ele se sentiu quando perdeu a pipa e como\n"
     ]
    }
   ],
   "source": [
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto: \"A água é essencial para a vida na Terra. Ela não apenas sustenta os organismos vivos, mas também participa de vários processos vitais, como o ciclo hidrológico, que afeta o clima global e as condições meteorológicas.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: A água é crucial para a vida e processos globais como o clima.\n",
    "\n",
    "Texto: \"As florestas tropicais são ricas em biodiversidade. Elas não só abrigam uma grande variedade de espécies animais e vegetais, mas também ajudam a regular o clima terrestre e são vitais para a manutenção de muitos ecossistemas.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: Florestas tropicais têm alta biodiversidade e são importantes para o clima e ecossistemas.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Texto: \"Sabemos onde realmente está a pipa em nosso coração? Não conseguiremos isso de novo se perdermos isso na vida, talvez nos arrependamos e nos redimamos, mas tudo isso parece ser tarde demais. Sempre que a pipa é lançada no céu, não deveríamos nos perguntar se realmente valorizamos o que temos?\n",
    "Cada um de nós fez algo na juventude que nos envergonhará no futuro. Essas coisas podem ser como uma sombra que o acompanha pelo resto da vida, de modo que você só consegue olhar para ela. Mas o tempo não vai mudar, os seus melhores esforços para compensar isso, como não é uma espécie de auto-ajuda? A beleza e a amizade da infância, por causa de uma negligência covarde e arruinada, se você tiver outra chance, você está disposto a se esforçar para recuperar aquele antigo eu?\n",
    "Mas as pessoas são assim, vivem sempre dentro de um determinado limite de tempo, onde o mundo pode estar alguns anos depois nem elas conseguem entender, mas é isso que não conseguimos superar. Para você, mil vezes, todo o corpo ainda estará justo, talvez isso seja a vida, a vida não é apenas fazer coisas que valem a pena!\"\n",
    "\n",
    "Pergunta: Resuma o texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto: \"A água é vital para todas as formas de vida conhecidas. A importância da água para os seres vivos é tão grande que ela é considerada uma substância essencial para a existência de vida.\"\n",
      "Pergunta: Resuma o texto.\n",
      "Resposta: A água é crucial para a vida de todos os seres vivos conhecidos.\n",
      "\n",
      "Texto: \"As florestas são essenciais para a saúde do nosso planeta. Elas fornecem oxigênio, abrigam biodiversidade, regulam o clima e oferecem recursos para a vida humana.\"\n",
      "Pergunta: Resuma o texto.\n",
      "Resposta: As florestas desempenham um papel vital no bem-estar do planeta e no suporte à vida.\n",
      "\n",
      "\n",
      "Q: \n",
      "Texto:  \"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. \n",
      "A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. \n",
      "A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
      "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. \n",
      "Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\"\n",
      "Fernando Pessoa\n",
      "\n",
      "Pergunta: Resuma o texto.\n",
      "\n",
      "A: A literatura é uma forma de arte que se distingue por sua capacidade de simular a vida. Enquanto as outras formas de arte, como a música, as artes visuais e as artes vivas, se envolvem diretamente com a vida, a literatura se afasta da vida por meio de sua capacidade de criar histórias e personagens fictícios.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Sumarização de Texto #################\n",
    "################# \"World Knowledge\" e Compreensão #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto: \"A água é vital para todas as formas de vida conhecidas. A importância da água para os seres vivos é tão grande que ela é considerada uma substância essencial para a existência de vida.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: A água é crucial para a vida de todos os seres vivos conhecidos.\n",
    "\n",
    "Texto: \"As florestas são essenciais para a saúde do nosso planeta. Elas fornecem oxigênio, abrigam biodiversidade, regulam o clima e oferecem recursos para a vida humana.\"\n",
    "Pergunta: Resuma o texto.\n",
    "Resposta: As florestas desempenham um papel vital no bem-estar do planeta e no suporte à vida.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Texto:  \"Escrever é esquecer. A literatura é a maneira mais agradável de ignorar a vida. \n",
    "A música embala, as artes visuais animam, as artes vivas (como a dança e a arte de representar) entretêm. \n",
    "A primeira, porém, afasta-se da vida por fazer dela um sono; as segundas, contudo, não se afastam da vida - umas porque usam de fórmulas visíveis e portanto vitais, outras porque vivem da mesma vida humana.\n",
    "Não é o caso da literatura. Essa simula a vida. Um romance é uma história do que nunca foi e um drama é um romance dado sem narrativa. \n",
    "Um poema é a expressão de ideias ou de sentimentos em linguagem que ninguém emprega, pois que ninguém fala em verso.\"\n",
    "Fernando Pessoa\n",
    "\n",
    "Pergunta: Resuma o texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################### TRANSFORMAÇÃO DE FORMATOS  ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa: Silva acredita em outra coisa. De acordo com sua pesquisa, as declarações anteriores sobre este assunto estão incorretas.\n",
      "Resposta: Acredito em outra coisa, disse Silva. De acordo com a minha pesquisa, as declarações anteriores sobre este assunto estão incorretas.\n",
      "\n",
      "\n",
      "Q: \n",
      "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa:\n",
      "Apesar dos desafios envolvidos, os pesquisadores continuam a defender as suas reivindicações.\n",
      "\n",
      "A: \n",
      "Apesar dos desafios envolvidos, continuo a defender as suas reivindicações.\n",
      "\n",
      "\n",
      "Q: \n",
      "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa:\n",
      "Apesar dos desafios envolvidos, os pesquisadores continuam a defender as suas reivindicações.\n",
      "\n",
      "A: \n",
      "Apesar dos\n"
     ]
    }
   ],
   "source": [
    "################# Converter uma frase na terceira pessoa  para a primeira pessoa #################\n",
    "\n",
    "context = \"\"\"\n",
    "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa: Silva acredita em outra coisa. De acordo com sua pesquisa, as declarações anteriores sobre este assunto estão incorretas.\n",
    "Resposta: Acredito em outra coisa, disse Silva. De acordo com a minha pesquisa, as declarações anteriores sobre este assunto estão incorretas.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Transforme a frase, que está na terceira pessoa, em frase em primeira pessoa:\n",
    "Apesar dos desafios envolvidos, os pesquisadores continuam a defender as suas reivindicações.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=100,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o parágrafo abaixo para o português brasileiro\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Machine Translation #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Tales from Shakespeare is an English children's book written by Charles Lamb and his sister Mary Lamb in 1807. The book is designed to make the stories of Shakespeare's plays familiar to the young. However, as noted in the author's Preface, \"[Shakespeare's] words are used whenever it seemed possible to bring them in; and in whatever has been added to give them the regular form of a connected story, diligent care has been taken to select such words as might least interrupt the effect of the beautiful English tongue in which he wrote: therefore, words introduced into our language since his time have been as far as possible avoided.\" \n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o parágrafo abaixo para o português brasileiro\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As Seller, an encyclopedia salesman, approached the grounds on which the house of Hermit, the hermit, was situated, he saw a sign that said, \"No salesmen. Trespassers will be prosecuted. Proceed at your own risk.\"\n",
      "Although Seller had not been invited to enter, he ignored the sign and drove up the driveway toward the house. As he rounded a curve, a powerful explosive charge buried in the driveway exploded, and Seller was injured. \n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Traduza o parágrafo para o português.\n",
      "\n",
      "\n",
      "A: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# TRANSORMAÇÃO DE FORMATOS  #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "As Seller, an encyclopedia salesman, approached the grounds on which the house of Hermit, the hermit, was situated, he saw a sign that said, \"No salesmen. Trespassers will be prosecuted. Proceed at your own risk.\"\n",
    "Although Seller had not been invited to enter, he ignored the sign and drove up the driveway toward the house. As he rounded a curve, a powerful explosive charge buried in the driveway exploded, and Seller was injured. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Traduza o parágrafo para o português.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Um pedestre atravessou a rua fora da faixa de pedestres e foi atingido por um carro. O motorista estava acima do limite de velocidade.\"\n",
      "Pergunta: O motorista é responsável pelos ferimentos do pedestre?\n",
      "(A) Sim, porque ele estava acima do limite de velocidade.\n",
      "(B) Não, porque o pedestre não estava na faixa.\n",
      "Resposta: (A) Sim, porque ele estava acima do limite de velocidade.\n",
      "Justificativa: Apesar de o pedestre não estar na faixa, o motorista tem a responsabilidade final de controlar a velocidade do veículo para evitar acidentes, o que é uma expectativa legal padrão.\n",
      "\n",
      "\"Um cliente escorrega e cai em uma loja onde um sinal de 'Piso Molhado' estava claramente visível.\"\n",
      "Pergunta: A loja é responsável pelos ferimentos do cliente?\n",
      "(A) Sim, porque é responsabilidade da loja manter um ambiente seguro.\n",
      "(B) Não, porque o sinal de aviso estava visível.\n",
      "Resposta: (B) Não, porque o sinal de aviso estava visível.\n",
      "Justificativa: A presença de um aviso claro reduz a responsabilidade da loja, pois isso significa que o cliente foi devidamente informado sobre o perigo potencial.\n",
      "\n",
      "\n",
      "Q: \n",
      "Can Seller recover damages from Hermit for his injuries?\n",
      "(A) Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\n",
      "(B) Yes, if Hermit was responsible for the explosive charge under the driveway.\n",
      "(C) No, because Seller ignored the sign, which warned him against proceeding further.\n",
      "(D) No, if Hermit reasonably feared that intruders would come and harm him or his family.\n",
      "\n",
      "Justifique a escolha da alternativa correta entre as quatro apresentadas (A, B, C ou D).\n",
      "\n",
      "A: Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\n",
      "\n",
      "Justificativa: A alternativa correta é A, pois a resposta correta é \"Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\" Isso significa que o vendedor pode recuperar os danos se Hermit não tivesse a intenção de causar dano aos intrusos, mas apenas de deter os intrusos.\n",
      "\n",
      "B: Yes, if Hermit was responsible for the explosive charge under the driveway.\n",
      "\n",
      "Justificativa: A alternativa correta é A, pois a resposta correta é \"Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\" Isso significa que o vendedor pode recuperar os danos se Hermit não tivesse a intenção de causar dano aos intrusos, mas apenas de deter os intrusos.\n",
      "\n",
      "C: No, because Seller ignored the sign, which warned him against proceeding further.\n"
     ]
    }
   ],
   "source": [
    "################# TRANSORMAÇÃO DE FORMATOS E ICL com self-reflection  #################\n",
    "\n",
    "context = \"\"\"\n",
    "\"Um pedestre atravessou a rua fora da faixa de pedestres e foi atingido por um carro. O motorista estava acima do limite de velocidade.\"\n",
    "Pergunta: O motorista é responsável pelos ferimentos do pedestre?\n",
    "(A) Sim, porque ele estava acima do limite de velocidade.\n",
    "(B) Não, porque o pedestre não estava na faixa.\n",
    "Resposta: (A) Sim, porque ele estava acima do limite de velocidade.\n",
    "Justificativa: Apesar de o pedestre não estar na faixa, o motorista tem a responsabilidade final de controlar a velocidade do veículo para evitar acidentes, o que é uma expectativa legal padrão.\n",
    "\n",
    "\"Um cliente escorrega e cai em uma loja onde um sinal de 'Piso Molhado' estava claramente visível.\"\n",
    "Pergunta: A loja é responsável pelos ferimentos do cliente?\n",
    "(A) Sim, porque é responsabilidade da loja manter um ambiente seguro.\n",
    "(B) Não, porque o sinal de aviso estava visível.\n",
    "Resposta: (B) Não, porque o sinal de aviso estava visível.\n",
    "Justificativa: A presença de um aviso claro reduz a responsabilidade da loja, pois isso significa que o cliente foi devidamente informado sobre o perigo potencial.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Can Seller recover damages from Hermit for his injuries?\n",
    "(A) Yes, unless Hermit, when he planted the charge, intended only to deter, not harm, intruders.\n",
    "(B) Yes, if Hermit was responsible for the explosive charge under the driveway.\n",
    "(C) No, because Seller ignored the sign, which warned him against proceeding further.\n",
    "(D) No, if Hermit reasonably feared that intruders would come and harm him or his family.\n",
    "\n",
    "Justifique a escolha da alternativa correta entre as quatro apresentadas (A, B, C ou D).\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Esta é uma tarefa de retirada de referências bibliográficas do texto. As referências estão escritas no formato \"[número]\". Exemplo: [10].\n",
      "\n",
      "Exemplos:\n",
      "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
      "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
      "Contexto: Ontem foi dia de ramos [32].\n",
      "Resposta: Ontem foi dia de ramos.\n",
      "\n",
      "\n",
      "Q: \n",
      "Contexto: José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
      "\n",
      "Remova do texeto as referências no formato \"[número]\" e escreva a Resposta:\n",
      "\n",
      "\n",
      "A: D. Pedro II passava os dias estudando, com apenas duas horas livres para recreação. Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama. Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai. Sua paixão pela leitura lhe permitiu assimilar qualquer informação. D. Pedro II não era um gênio, mas inteligente e com grande capacidade para acumular conhecimento facilmente.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################# Retirar Referências do Texto #################\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "Esta é uma tarefa de retirada de referências bibliográficas do texto. As referências estão escritas no formato \"[número]\". Exemplo: [10].\n",
    "\n",
    "Exemplos:\n",
    "Contexto: Joao da silva era carpinteiro [1][24]. Estudou na escola santa amelia [17] até os 18 anos de idade.\n",
    "Resposta: Joao da silva era carpinteiro. Estudou na escola santa amelia até os 18 anos de idade.\n",
    "Contexto: Ontem foi dia de ramos [32].\n",
    "Resposta: Ontem foi dia de ramos.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "question = \"\"\"\n",
    "Contexto: José Bonifácio foi destituído de sua posição em dezembro de 1833 e substituído por outro tutor.[31][32][33] D. Pedro II passava os dias estudando,[34] com apenas duas horas livres para recreação.[35] Acordava às 06h30 da manhã e começava seus estudos às sete, continuando até as dez da noite, quando ia para cama.[36] Tomou-se grande cuidado em sua educação para formar valores e personalidade diferente da impulsividade e irresponsabilidade demonstradas por seu pai.[31][37] Sua paixão pela leitura lhe permitiu assimilar qualquer informação.[38] D. Pedro II não era um gênio,[39] mas inteligente[40] e com grande capacidade para acumular conhecimento facilmente.[41]\n",
    "\n",
    "Remova do texeto as referências no formato \"[número]\" e escreva a Resposta:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################# EXTRAÇÃO DE CONTEÚDO  #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
      "\n",
      "Pergunta: Qual é o maior sistema de recifes de corais do mundo?\n",
      "Resposta: A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo.\n",
      "\n",
      "Pergunta: Quantas espécies de peixes podem ser encontradas na Grande Barreira de Corais?\n",
      "Resposta: Mais de 1.500 espécies de peixes podem ser encontradas na Grande Barreira de Corais.\n",
      "\n",
      "\n",
      "Q: \n",
      "Dada a descrição do contexto acima:\n",
      "O que é a Grande Barreira de Corais e onde ela está localizada?\n",
      "\n",
      "A: \n",
      "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália.\n",
      "\n",
      "Q: \n",
      "Qual é a importância da Grande Barreira de Corais para a vida marinha?\n",
      "\n",
      "A: \n",
      "A Grande Barreira de Corais abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes,\n"
     ]
    }
   ],
   "source": [
    "################# Extração de Conteúdo #################\n",
    "\n",
    "context = \"\"\"\n",
    "A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo, composto por mais de 2.900 recifes individuais e 900 ilhas que se estendem \\\n",
    "por mais de 2.300 quilômetros. Está localizado no Mar de Coral, na costa da Austrália. \\\n",
    "O recife abriga uma grande variedade de vida marinha, incluindo mais de 1.500 espécies de peixes, 600 tipos de corais e inúmeras outras espécies.\n",
    "\n",
    "Pergunta: Qual é o maior sistema de recifes de corais do mundo?\n",
    "Resposta: A Grande Barreira de Corais é o maior sistema de recifes de corais do mundo.\n",
    "\n",
    "Pergunta: Quantas espécies de peixes podem ser encontradas na Grande Barreira de Corais?\n",
    "Resposta: Mais de 1.500 espécies de peixes podem ser encontradas na Grande Barreira de Corais.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Dada a descrição do contexto acima:\n",
    "O que é a Grande Barreira de Corais e onde ela está localizada?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Geração da resposta\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=160,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto de Referência:\n",
      "O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, com uma população estimada em cerca de 211 milhões de pessoas em 2021. O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
      "Pergunta: Qual a dimensão do Brasil em área?\n",
      "Resposta: 8.515.767 quilômetros quadrados.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Texto de Referência:\n",
      "O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, com uma população estimada em cerca de 211 milhões de pessoas em 2021. O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
      "Pergunta: Qual é a posição do Brasil em termos de tamanho de território e população, e qual é a principal característica natural mencionada no texto?\n",
      "\n",
      "A: O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, com uma população estimada em cerca de 211 milhões de pessoas em 2021. O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Texto de Referência:\n",
      "O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, com uma população estimada em cerca de 211 milhões de pessoas em 2021. O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
      "Pergunta: Qual é a posição do\n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Informações Específicas #################\n",
    "\n",
    "context = \"\"\"\n",
    "Texto de Referência:\n",
    "O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, \\\n",
    "com uma população estimada em cerca de 211 milhões de pessoas em 2021. \\\n",
    "O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
    "Pergunta: Qual a dimensão do Brasil em área?\n",
    "Resposta: 8.515.767 quilômetros quadrados.\n",
    "\n",
    "\"\"\"\n",
    "question = \"\"\"\n",
    "Texto de Referência:\n",
    "O Brasil é o quinto maior país do mundo em território, com uma área total de 8.515.767 quilômetros quadrados. É também o quinto país mais populoso do mundo, com uma população estimada em cerca de 211 milhões de pessoas em 2021. O país é conhecido por sua diversidade cultural e natural, incluindo a floresta Amazônica, que é a maior floresta tropical do mundo.\n",
    "Pergunta: Qual é a posição do Brasil em termos de tamanho de território e população, e qual é a principal característica natural mencionada no texto?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto:\n",
      "\"A Microsoft, fundada por Bill Gates e Paul Allen, está sediada em Redmond, Washington.\"\n",
      "\n",
      "Análise:\n",
      "{\n",
      "  \"pessoa\": [\"Bill Gates\", \"Paul Allen\"],\n",
      "  \"lugar\": [\"Redmond\", \"Washington\"],\n",
      "  \"organização\": [\"Microsoft\"]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
      "\n",
      "A:\n",
      "{\n",
      "  \"pessoa\": [\"Tim Cook\"],\n",
      "  \"lugar\": [\"Cupertino\", \"California\"],\n",
      "  \"organização\": [\"Apple\"]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
      "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
      "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
      "\n",
      "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
      "\n",
      "A:\n",
      "{\n",
      "  \"pessoa\": [\"Tim Cook\"],\n",
      "  \"lugar\": [\"Cupertino\", \"California\"],\n",
      "  \"organização\": [\"Apple\"]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q: \n"
     ]
    }
   ],
   "source": [
    "################# Reconhecimento e Extração de Entidades #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"A Microsoft, fundada por Bill Gates e Paul Allen, está sediada em Redmond, Washington.\"\n",
    "\n",
    "Análise:\n",
    "{\n",
    "  \"pessoa\": [\"Bill Gates\", \"Paul Allen\"],\n",
    "  \"lugar\": [\"Redmond\", \"Washington\"],\n",
    "  \"organização\": [\"Microsoft\"]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "O evento da Apple aconteceu no Steve Jobs Theatre em Cupertino, Califórnia. \n",
    "Tim Cook, CEO da Apple, apresentou o novo iPhone 15, que estará disponível para pré-encomenda a partir da próxima semana. \n",
    "O telefone vem em várias cores, apresenta um processador mais rápido e maior duração da bateria.\n",
    "\n",
    "Analise o parágrafo, identifique em formato JSON Somente as entidades (\"entities\") dos tipos \"pessoa\", \"lugar\" e \"organização\" mencionadas no texto.\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto:\n",
      "\"Mark Zuckerberg criou o Facebook enquanto estudava em Harvard.\"\n",
      "Análise de Relacionamento:\n",
      "{\n",
      "  \"entidade1\": \"Mark Zuckerberg\",\n",
      "  \"entidade2\": \"Facebook\",\n",
      "  \"relacionamento\": \"criou\"\n",
      "}\n",
      "\n",
      "Texto:\n",
      "\"Jeff Bezos fundou a Amazon em 1994 e transformou-a numa das maiores empresas de comércio eletrônico do mundo.\"\n",
      "Análise de Relacionamento:\n",
      "{\n",
      "  \"entidade1\": \"Jeff Bezos\",\n",
      "  \"entidade2\": \"Amazon\",\n",
      "  \"relacionamento\": \"fundou\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q: \n",
      "Elon Musk fundou a SpaceX em 2002 com o objetivo de reduzir os custos de transporte espacial e possibilitar a colonização de Marte.\n",
      "\n",
      "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
      "\n",
      "A:\n",
      "{\n",
      "  \"entidade1\": \"Elon Musk\",\n",
      "  \"entidade2\": \"SpaceX\",\n",
      "  \"relacionamento\": \"fundou\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q:\n",
      "\n",
      "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
      "\n",
      "A:\n",
      "{\n",
      "  \"entidade1\": \"Elon Musk\",\n",
      "  \"entidade2\": \"SpaceX\",\n",
      "  \"relacionamento\": \"fundou\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q:\n",
      "\n",
      "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
      "\n",
      "A:\n",
      "{\n",
      "  \"entidade1\": \"Elon Musk\",\n",
      "  \"entidade2\": \"SpaceX\",\n",
      "  \"relacionamento\": \"fundou\"\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Q:\n",
      "\n",
      "Analise a frase e extraia o relacionamento entre duas entidades e,\n"
     ]
    }
   ],
   "source": [
    "################# Extração de Relações #################\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "Texto:\n",
    "\"Mark Zuckerberg criou o Facebook enquanto estudava em Harvard.\"\n",
    "Análise de Relacionamento:\n",
    "{\n",
    "  \"entidade1\": \"Mark Zuckerberg\",\n",
    "  \"entidade2\": \"Facebook\",\n",
    "  \"relacionamento\": \"criou\"\n",
    "}\n",
    "\n",
    "Texto:\n",
    "\"Jeff Bezos fundou a Amazon em 1994 e transformou-a numa das maiores empresas de comércio eletrônico do mundo.\"\n",
    "Análise de Relacionamento:\n",
    "{\n",
    "  \"entidade1\": \"Jeff Bezos\",\n",
    "  \"entidade2\": \"Amazon\",\n",
    "  \"relacionamento\": \"fundou\"\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Elon Musk fundou a SpaceX em 2002 com o objetivo de reduzir os custos de transporte espacial e possibilitar a colonização de Marte.\n",
    "\n",
    "Analise a frase e extraia o relacionamento entre duas entidades e, em seguida, produza os resultados indicando: entities e relatiohships entre estas no formato JSON\n",
    "\"\"\"\n",
    "\n",
    "# Combinação do contexto e da questão para formar o prompt\n",
    "prompt = context + \"\\n\\nQ: \" + question + \"\\nA:\"\n",
    "\n",
    "# Tokenização do prompt\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Extrair os tensores de input_ids e da attention_mask\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "input_ids = input_ids.to('cuda')\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Outputs - Passar ambos para o modelo e gerar a resposta (generate_ids = outputs)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        #do_sample=True,\n",
    "        #top_k=40,\n",
    "    )\n",
    "\n",
    "# Decodificação e impressão da resposta (answer = last_hidden_states)\n",
    "last_hidden_states = tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "answer = last_hidden_states\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
